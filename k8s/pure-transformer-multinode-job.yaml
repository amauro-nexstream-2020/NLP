apiVersion: v1
kind: Service
metadata:
  name: pure-transformer-multinode-master
  namespace: ucsdfutures
spec:
  clusterIP: None  # Headless service for peer discovery
  selector:
    job-name: pure-transformer-multinode
    role: master
  ports:
  - port: 29500
    name: torch-dist
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pure-transformer-multinode
  namespace: ucsdfutures
spec:
  parallelism: 2  # 2 nodes
  completions: 2
  completionMode: Indexed
  template:
    metadata:
      labels:
        job-name: pure-transformer-multinode
    spec:
      restartPolicy: OnFailure
      
      # Share checkpoints and cache across pods
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: pure-transformer-checkpoints
      - name: hf-cache
        persistentVolumeClaim:
          claimName: pure-transformer-hf-cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: secret-volume
        secret:
          secretName: hybrid-llm-secrets
      
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.01-py3
        
        # Multi-node DDP environment variables
        env:
        - name: MASTER_ADDR
          value: "pure-transformer-multinode-master"
        - name: MASTER_PORT
          value: "29500"
        - name: WORLD_SIZE
          value: "2"  # 2 nodes
        - name: NODE_RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "0"  # Enable InfiniBand
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        # HuggingFace cache
        - name: HF_HOME
          value: /hf-cache
        - name: HF_DATASETS_CACHE
          value: /hf-cache/datasets
        - name: TRANSFORMERS_CACHE
          value: /hf-cache/transformers
        # W&B credentials
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: hybrid-llm-secrets
              key: wandb-api-key
        - name: WANDB_PROJECT
          value: "pure-transformer"
        - name: WANDB_NAME
          value: "xlarge-multinode-100B"
        
        workingDir: /workspace
        
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -ex
          
          echo "======================================"
          echo "MULTI-NODE DDP TRAINING - Node ${NODE_RANK}"
          echo "======================================"
          echo "Master: ${MASTER_ADDR}:${MASTER_PORT}"
          echo "World size: ${WORLD_SIZE}"
          echo "Node rank: ${NODE_RANK}"
          
          # Clone repo
          cd /workspace
          if [ ! -d "NLP" ]; then
            git clone https://github.com/amauro-nexstream-2020/NLP.git
            cd NLP
          else
            cd NLP
            git pull
          fi
          
          # Install dependencies
          pip install --no-cache-dir -q \
            lightning \
            transformers \
            datasets \
            tokenizers \
            wandb \
            tensorboard \
            numpy \
            tqdm
          
          # Install flash-attention for 2x speedup
          pip install --no-cache-dir flash-attn --no-build-isolation || echo "Flash attention install failed, continuing without it"
          
          # Label this node as master if rank 0
          if [ "${NODE_RANK}" = "0" ]; then
            export ROLE=master
            echo "This is the MASTER node"
          else
            export ROLE=worker
            echo "This is a WORKER node"
          fi
          
          echo ""
          echo "======================================"
          echo "Starting training on node ${NODE_RANK}"
          echo "======================================"
          
          # Run multi-node training
          cd pure_transformer
          python train_multigpu.py \
            --model xlarge \
            --devices 4 \
            --nodes 2 \
            --strategy ddp \
            --precision bf16-mixed \
            --total-tokens 100000000000 \
            --global-batch-size 524288 \
            --micro-batch-size 16 \
            --seq-length 2048 \
            --learning-rate 3e-4 \
            --min-lr 3e-5 \
            --weight-decay 0.1 \
            --warmup-tokens 100000000 \
            --num-workers 12 \
            --checkpoint-dir /checkpoints \
            --save-every-n-steps 1000 \
            --log-every-n-steps 10 \
            --use-wandb \
            --wandb-project pure-transformer \
            --fineweb-subset sample-100BT \
            --fineweb-prob 0.65 \
            --finepdf-prob 0.34 \
            --usmle-prob 0.01 \
            --seed 42
          
          echo ""
          echo "======================================"
          echo "Training completed on node ${NODE_RANK}"
          echo "======================================"
        
        resources:
          requests:
            memory: "120Gi"
            cpu: "24"
            nvidia.com/gpu: "4"  # 4 GPUs per node (any NVIDIA GPU)
          limits:
            memory: "120Gi"
            cpu: "24"
            nvidia.com/gpu: "4"
        
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        - name: hf-cache
          mountPath: /hf-cache
        - name: dshm
          mountPath: /dev/shm
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: nautilus.io/hardware
        operator: Exists
        effect: NoSchedule
