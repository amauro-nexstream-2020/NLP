---
# PRODUCTION Single-Node 8xA100 Training Job
# Target: 24 hours training time
# Dataset Mix: 85% FineWeb, 14.5% FinePDF, 0.5% USMLE
#
apiVersion: batch/v1
kind: Job
metadata:
  name: pure-transformer-8xa100-prod
  namespace: ucsdfutures
spec:
  parallelism: 1
  completions: 1
  backoffLimit: 2
  template:
    metadata:
      labels:
        job-name: pure-transformer-8xa100-prod
    spec:
      restartPolicy: OnFailure
      
      # Try multiple nodes with 8 A100s
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - node-1-1.sdsc.optiputer.net
                - node-1-3.sdsc.optiputer.net
                - node-2-1.sdsc.optiputer.net
                - node-2-2.sdsc.optiputer.net
                - node-2-3.sdsc.optiputer.net
      
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: pure-transformer-checkpoints
      - name: hf-cache
        persistentVolumeClaim:
          claimName: pure-transformer-hf-cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 64Gi
      - name: secret-volume
        secret:
          secretName: hybrid-llm-secrets
      
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.09-py3
        
        env:
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "0"
        - name: NCCL_P2P_LEVEL
          value: "NVL"
        - name: HF_HOME
          value: "/hf-cache"
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: hybrid-llm-secrets
              key: wandb-api-key
              optional: true
        - name: WANDB_NAME
          value: "pure-transformer-1.5b-8xa100-24h"
        
        command:
        - /bin/bash
        - -c
        - |
          set -exo pipefail
          
          echo "======================================"
          echo "PRODUCTION 8xA100 Training - 24 Hours"
          echo "======================================"
          echo "Node: $(hostname)"
          echo "Dataset Mix: 85% FineWeb, 14.5% FinePDF, 0.5% USMLE"
          echo "Target: 50B tokens in 24 hours"
          
          # GPU info
          nvidia-smi
          nvidia-smi topo -m || true
          
          cd /workspace
          if [ ! -d "NLP" ]; then
            git clone --depth 1 --branch pure-transformer https://github.com/amauro-nexstream-2020/NLP.git
          else
            cd NLP
            git fetch origin pure-transformer
            git checkout pure-transformer
            git reset --hard origin/pure-transformer
            cd ..
          fi
          cd NLP
          
          # Install dependencies
          echo "Installing dependencies..."
          pip install --no-cache-dir -q --upgrade typing_extensions pip
          pip install --no-cache-dir -q lightning transformers datasets tokenizers wandb tensorboard
          
          # Try to install flash-attn (faster training)
          echo "Checking flash-attention..."
          pip install --no-cache-dir flash-attn --no-build-isolation || echo "Flash-attn not available, will use SDPA"
          
          # Verify environment
          python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'GPUs: {torch.cuda.device_count()}')"
          python -c "import flash_attn; print(f'flash-attn: {flash_attn.__version__}')" 2>/dev/null || echo "flash-attn: not available (using SDPA)"
          
          LOGFILE=/checkpoints/training_8xa100_prod.log
          
          echo ""
          echo "======================================"
          echo "Starting Training..."
          echo "======================================"
          
          # Training with optimized settings for 24-hour run
          # 50B tokens / 24 hours / 8 GPUs / 2048 seq_len / 32 batch_size
          # = ~30,000 steps
          PYTHONPATH=$(pwd) python -m pure_transformer.train_multigpu \
            --model xlarge \
            --devices 8 \
            --nodes 1 \
            --strategy ddp \
            --precision bf16-mixed \
            --total-tokens 50000000000 \
            --global-batch-size 2097152 \
            --micro-batch-size 32 \
            --seq-length 2048 \
            --learning-rate 3e-4 \
            --min-lr 3e-5 \
            --weight-decay 0.1 \
            --warmup-tokens 500000000 \
            --num-workers 8 \
            --checkpoint-dir /checkpoints \
            --save-every-n-steps 500 \
            --log-every-n-steps 10 \
            --use-wandb \
            --wandb-project pure-transformer \
            --fineweb-subset sample-100BT \
            --fineweb-prob 0.85 \
            --finepdf-prob 0.145 \
            --usmle-prob 0.005 \
            --seed 42 2>&1 | tee ${LOGFILE}
          
          echo ""
          echo "======================================"
          echo "Training completed!"
          echo "======================================"
        
        resources:
          requests:
            memory: "320Gi"
            cpu: "32"
            nvidia.com/a100: "8"
          limits:
            memory: "400Gi"
            cpu: "64"
            nvidia.com/a100: "8"
        
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        - name: hf-cache
          mountPath: /hf-cache
        - name: dshm
          mountPath: /dev/shm
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: nautilus.io/hardware
        operator: Exists
        effect: NoSchedule
