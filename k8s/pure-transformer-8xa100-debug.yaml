apiVersion: batch/v1
kind: Job
metadata:
  name: pure-transformer-8xa100-debug
  namespace: ucsdfutures
spec:
  parallelism: 2
  completions: 2
  backoffLimit: 20
  template:
    metadata:
      labels:
        job-name: pure-transformer-8xa100-debug
    spec:
      restartPolicy: OnFailure
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: pure-transformer-checkpoints
      - name: hf-cache
        persistentVolumeClaim:
          claimName: pure-transformer-hf-cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: secret-volume
        secret:
          secretName: hybrid-llm-secrets
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.01-py3
        env:
        - name: MASTER_ADDR
          value: "pure-transformer-8xa100-master"
        - name: MASTER_PORT
          value: "29500"
        - name: WORLD_SIZE
          value: "2"
        - name: NODE_RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: HF_HOME
          value: /hf-cache
        - name: HF_DATASETS_CACHE
          value: /hf-cache/datasets
        - name: TRANSFORMERS_CACHE
          value: /hf-cache/transformers
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: hybrid-llm-secrets
              key: wandb-api-key
        - name: WANDB_PROJECT
          value: "pure-transformer"
        - name: WANDB_NAME
          value: "xlarge-8xa100-debug-1000"
        workingDir: /workspace
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -ex
          echo "DEBUG RUN: Node rank ${NODE_RANK}"
          cd /workspace
          if [ ! -d "NLP" ]; then git clone https://github.com/amauro-nexstream-2020/NLP.git; fi
          cd NLP || true
          pip install --no-cache-dir -q lightning transformers datasets tokenizers wandb tensorboard numpy tqdm || true
          python pure_transformer/train_multigpu.py \
            --model xlarge \
            --devices 2 \
            --nodes 2 \
            --strategy ddp \
            --total-tokens 1000 \
            --global-batch-size 1024 \
            --micro-batch-size 4 \
            --precision bf16-mixed 2>&1 | tee /checkpoints/debug-${NODE_RANK}.log || (echo "TRAIN FAILED - see /checkpoints/debug-${NODE_RANK}.log" && tail -n 200 /checkpoints/debug-${NODE_RANK}.log && sleep 3600)

        resources:
          requests:
            memory: "60Gi"
            cpu: "12"
            nvidia.com/a100: "2"
          limits:
            memory: "60Gi"
            cpu: "12"
            nvidia.com/a100: "2"
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        - name: hf-cache
          mountPath: /hf-cache
        - name: dshm
          mountPath: /dev/shm
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: nautilus.io/hardware
        operator: Exists
        effect: NoSchedule
