---
# Headless service for multi-node coordination
apiVersion: v1
kind: Service
metadata:
  name: pure-transformer-2x4-master
  namespace: ucsdfutures
spec:
  clusterIP: None
  selector:
    job-name: pure-transformer-2x4
  ports:
  - port: 29500
    name: torch-dist
---
# Job with 2 nodes: 4 + 4 A100s = 8 total A100s
apiVersion: batch/v1
kind: Job
metadata:
  name: pure-transformer-2x4
  namespace: ucsdfutures
spec:
  parallelism: 2  # 2 nodes with 4 A100s each
  completions: 2
  backoffLimit: 20
  completionMode: Indexed
  template:
    metadata:
      labels:
        job-name: pure-transformer-2x4
    spec:
      restartPolicy: OnFailure
      
      volumes:
      - name: checkpoints
        persistentVolumeClaim:
          claimName: pure-transformer-checkpoints
      - name: hf-cache
        persistentVolumeClaim:
          claimName: pure-transformer-hf-cache
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 64Gi
      - name: secret-volume
        secret:
          secretName: hybrid-llm-secrets
      
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.01-py3
        
        env:
        - name: MASTER_ADDR
          value: "pure-transformer-2x4-master"
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MASTER_PORT
          value: "29500"
        - name: WORLD_SIZE
          # 2 nodes * 4 GPUs = 8 total ranks? 
          # PyTorch Lightning DDP strategy usually expects WORLD_SIZE to be total nodes if using their launcher, 
          # or total GPUs if using torchrun. 
          # The script uses Lightning Trainer(num_nodes=NODES, devices=DEVICES).
          # So WORLD_SIZE here is likely just for information or manual usage.
          # We'll set it to 2 (nodes) as per the script logic.
          value: "2"
        - name: NODE_RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['batch.kubernetes.io/job-completion-index']
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "1"  # Disable InfiniBand (nodes are geographically distributed)
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: NCCL_P2P_DISABLE
          value: "1"  # Disable P2P for cross-datacenter
        - name: NCCL_NET_GDR_LEVEL
          value: "0"  # Disable GPU Direct RDMA for WAN
        - name: NCCL_BUFFSIZE
          value: "4194304"  # 4MB buffer for higher latency networks
        - name: NCCL_MIN_NCHANNELS
          value: "4"  # More channels for bandwidth
        - name: HF_HOME
          value: /hf-cache
        - name: HF_DATASETS_CACHE
          value: /hf-cache/datasets
        - name: TRANSFORMERS_CACHE
          value: /hf-cache/transformers
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: hybrid-llm-secrets
              key: wandb-api-key
        - name: WANDB_PROJECT
          value: "pure-transformer"
        - name: WANDB_NAME
          value: "xlarge-2x4-8xa100"
        - name: SKIP_WANDB
          value: "false"
        # GPU Performance Optimization
        - name: CUDA_DEVICE_MAX_CONNECTIONS
          value: "1"  # Serialize kernel launches for better utilization
        - name: NCCL_NSOCKS_PERTHREAD
          value: "4"  # More sockets per thread for bandwidth
        - name: NCCL_SOCKET_NTHREADS
          value: "4"  # More threads for socket operations
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:512"  # Optimize memory allocator
        
        workingDir: /workspace
        
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -ex
          
          echo "======================================"
          echo "2x4 A100 HYBRID TRAINING - Node ${NODE_RANK}"
          echo "======================================"
          echo "Master: ${MASTER_ADDR}:${MASTER_PORT}"
          echo "World size: ${WORLD_SIZE} nodes"
          echo "This node GPUs: 4 A100s"
          echo "Node rank: ${NODE_RANK}"
          
          cd /workspace
          if [ ! -d "NLP" ]; then
            git clone --depth 1 --branch pure-transformer https://github.com/amauro-nexstream-2020/NLP.git
          else
            cd NLP
            git checkout pure-transformer || true
            git pull || true
            cd ..
          fi
          cd NLP

          # Ensure typing_extensions (used by wandb) is upgraded to a version that provides TypeIs
          pip install --no-cache-dir -q --upgrade typing_extensions || true
          pip install --no-cache-dir -q lightning transformers datasets tokenizers wandb tensorboard numpy tqdm || true
          pip install --no-cache-dir flash-attn --no-build-isolation || true

          if [ "${NODE_RANK}" = "0" ]; then
            export ROLE=master
            echo ">>> MASTER NODE (4 A100s)"
            # Export MASTER_ADDR for the master and write master IP to a shared checkpoint file for workers to discover
            export MASTER_ADDR=${POD_IP}
            # remove any previous master file then write current master ip
            rm -f /checkpoints/master.ip || true
            echo "${POD_IP}" > /checkpoints/master.ip
            sync
          else
            export ROLE=worker
            echo ">>> WORKER NODE (4 A100s)"
          fi

          DEVICES=4
          NODES=2
          LOGFILE=/checkpoints/training_node_${NODE_RANK}.log
          # Worker nodes should wait for the master IP file to be visible on the shared PVC
          if [ "${NODE_RANK}" != "0" ]; then
            echo "Waiting for /checkpoints/master.ip to be created by master..."
            COUNT=0
            while [ ! -f /checkpoints/master.ip ]; do
              sleep 1
              COUNT=$((COUNT+1))
              if [ $COUNT -gt 300 ]; then
                echo "Timed out waiting for master file; attempting DNS fallback"
                break
              fi
            done
            if [ -f /checkpoints/master.ip ]; then
              # Retry read and test for reachability for up to 60s
              COUNT=0
              export MASTER_ADDR=$(cat /checkpoints/master.ip)
              echo "Discovered MASTER_ADDR from PVC: ${MASTER_ADDR}"
              while ! timeout 2 bash -c "</dev/tcp/${MASTER_ADDR}/${MASTER_PORT}" >/dev/null 2>&1; do
                COUNT=$((COUNT+1))
                if [ $COUNT -gt 60 ]; then
                  echo "Timeout waiting for MASTER ${MASTER_ADDR}:${MASTER_PORT} to respond; will try DNS fallback"
                  break
                fi
                echo "Waiting for MASTER ${MASTER_ADDR}:${MASTER_PORT} (attempt ${COUNT})..."
                sleep 1
                # re-read the file in case master updated it
                export MASTER_ADDR=$(cat /checkpoints/master.ip)
              done
            else
              # Try DNS fallback (may return multiple IPs; take first)
              MASTER_ADDR=$(getent hosts pure-transformer-2x4-master | awk '{print $1}' | head -n1 || true)
              echo "DNS fallback MASTER_ADDR: ${MASTER_ADDR}"
            fi
          fi

          PYTHONPATH=$(pwd) python -m pure_transformer.train_multigpu \
            --model xlarge \
            --devices ${DEVICES} \
            --nodes ${NODES} \
            --strategy ddp \
            --precision bf16-mixed \
            --total-tokens 69000000000 \
            --global-batch-size 2097152 \
            --micro-batch-size 32 \
            --seq-length 2048 \
            --learning-rate 3e-4 \
            --min-lr 3e-5 \
            --weight-decay 0.1 \
            --warmup-tokens 69000000 \
            --num-workers 12 \
            --checkpoint-dir /checkpoints \
            --save-every-n-steps 500 \
            --log-every-n-steps 10 \
            $(if [ "x${SKIP_WANDB}" = "xtrue" ]; then echo ""; else echo "--use-wandb --wandb-project pure-transformer"; fi) \
            --fineweb-subset sample-100BT \
            --fineweb-prob 0.65 \
            --finepdf-prob 0.34 \
            --usmle-prob 0.01 \
            --seed 42 2>&1 | tee ${LOGFILE} || (echo "TRAIN FAILED" >> ${LOGFILE} && tail -n 200 ${LOGFILE} && sleep 3600)
          
          echo ""
          echo "======================================"
          echo "Training completed on node ${NODE_RANK}"
          echo "======================================"
        
        resources:
          requests:
            memory: "200Gi"
            cpu: "32"
            nvidia.com/a100: "4"  # 4 A100s per node
          limits:
            memory: "200Gi"
            cpu: "32"
            nvidia.com/a100: "4"
        
        volumeMounts:
        - name: checkpoints
          mountPath: /checkpoints
        - name: hf-cache
          mountPath: /hf-cache
        - name: dshm
          mountPath: /dev/shm
        - name: secret-volume
          mountPath: /etc/secrets
          readOnly: true
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: job-name
                operator: In
                values:
                - pure-transformer-2x4
            topologyKey: kubernetes.io/hostname
      
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: nautilus.io/hardware
        operator: Exists
        effect: NoSchedule
