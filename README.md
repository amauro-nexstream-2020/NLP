# Decoder-Only Transformer LLM Course

A modular, educational implementation of a decoder-only transformer-based Large Language Model (LLM) for both general-purpose and domain-specific tasks.

## ğŸ¯ Purpose

This project provides a hands-on, notebook-based course for understanding and implementing a transformer-based LLM from scratch. The system is designed for:
- Educational purposes (learning transformers step-by-step)
- Research contexts (domain-specific fine-tuning)
- Tasks: Q&A, summarization, dialogue, biomedical/chemical sequence analysis

**Target Model Size**: â‰¤1B parameters (training feasibility)

## ğŸ“š Course Structure

The repository is organized as a series of Jupyter notebooks, each focusing on a specific component:

### Core Notebooks (Sequential Learning Path)

1. **`01_tokenizer.ipynb`** - Tokenization & Vocabulary
   - BPE/WordPiece implementation
   - Vocabulary building
   - Encoding/decoding text

2. **`02_embeddings.ipynb`** - Token & Positional Embeddings
   - Learned token embeddings
   - Positional encoding strategies
   - Embedding dimension analysis

3. **`03_attention.ipynb`** - Self-Attention Mechanism
   - Scaled dot-product attention
   - Multi-head attention
   - Causal masking for autoregressive generation

4. **`04_transformer_block.ipynb`** - Decoder Block
   - Layer normalization
   - Feed-forward networks
   - Residual connections
   - Complete transformer block assembly

5. **`05_model.ipynb`** - Full Model Architecture
   - Stacking decoder blocks
   - Output projection layer
   - Model initialization strategies

6. **`06_training.ipynb`** - Training Pipeline
   - Data loading & batching
   - Cross-entropy loss
   - Optimization (AdamW)
   - Learning rate scheduling
   - Checkpointing

7. **`07_generation.ipynb`** - Text Generation
   - Autoregressive sampling
   - Top-k and nucleus (top-p) sampling
   - Temperature control
   - Beam search (optional)

8. **`08_evaluation.ipynb`** - Model Evaluation
   - Perplexity calculation
   - Benchmark datasets
   - Validation metrics

9. **`09_fine_tuning.ipynb`** - Domain-Specific Fine-Tuning
   - Transfer learning strategies
   - Domain adaptation (biomedical, Q&A)
   - LoRA (parameter-efficient fine-tuning)

### Supplementary Notebooks

- **`data_preparation.ipynb`** - Dataset preprocessing
  - Textbook-quality datasets
  - Q&A dataset formatting
  - Domain-specific data (amino acids, medical)
  
- **`model_analysis.ipynb`** - Model inspection & visualization
  - Attention pattern visualization
  - Weight analysis
  - Embedding space exploration

- **`deployment.ipynb`** - Model deployment
  - Inference optimization
  - API creation
  - Simple web interface

## ğŸ—ï¸ System Architecture

```
Input Text
    â†“
[Tokenizer] â†’ Token IDs
    â†“
[Embedding Layer] â†’ Token Embeddings + Positional Encoding
    â†“
[Decoder Stack]
    â”œâ”€ Multi-Head Self-Attention (Causal)
    â”œâ”€ Layer Normalization
    â”œâ”€ Feed-Forward Network
    â””â”€ Residual Connections
    â†“
[Output Projection] â†’ Logits
    â†“
[Softmax] â†’ Probability Distribution
    â†“
Generated Text
```

## ğŸ“‹ Requirements

### Functional Requirements
- **FR001**: Decoder-only transformer architecture
- **FR002**: Support for general-purpose and domain-specific datasets
- **FR003**: Custom/Hugging Face tokenizer integration
- **FR004**: Coherent Q&A response generation
- **FR005**: Modular training pipelines (Colab, JupyterHub, Nautilus)
- **FR006**: Cross-entropy loss optimization
- **FR007**: Fine-tuning capabilities for domain-specific applications
- **FR008**: Comprehensive documentation

### Non-Functional Requirements
- **NFR001**: Training within HPC/Colab session limits
- **NFR002**: Reproducible experiments via version control
- **NFR003**: Maintained documentation
- **NFR004**: Extensible for multimodal expansion
- **NFR005**: Baseline performance comparable to NanoGPT

## ğŸš€ Getting Started

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd NLP

# Install dependencies
pip install -r requirements.txt
```

### Quick Start

1. Start with `01_tokenizer.ipynb` to understand tokenization
2. Progress sequentially through notebooks 02-05 to build the model
3. Use `06_training.ipynb` to train on your dataset
4. Experiment with `07_generation.ipynb` for text generation
5. Fine-tune for specific domains using `09_fine_tuning.ipynb`

## ğŸ’» Training Infrastructure

- **Nautilus**: NSF-funded NRP clusters (recommended for large-scale training)
- **Google Colab**: Free GPU access for experimentation
- **Local**: CPU/GPU training for small models

## ğŸ“Š Datasets

- **General-purpose**: Textbook-quality datasets, OpenWebText
- **Q&A**: SQuAD, Natural Questions
- **Domain-specific**: 
  - Biomedical: PubMed abstracts, protein sequences
  - Chemical: SMILES strings, molecular descriptions

## ğŸ§ª Testing & Validation

### Test Suite
1. **Sanity Check**: Training on small Q&A dataset
2. **Generation Test**: Coherent responses on validation prompts
3. **Performance Test**: Baseline perplexity/accuracy on textbook datasets
4. **Domain Test**: Fine-tuned model performance on specialized data
5. **Documentation Review**: Completeness check

## ğŸ“ Project Structure

```
NLP/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_configs.py      # Model architecture configurations
â”‚   â””â”€â”€ training_configs.py   # Training hyperparameters
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_tokenizer.ipynb
â”‚   â”œâ”€â”€ 02_embeddings.ipynb
â”‚   â”œâ”€â”€ 03_attention.ipynb
â”‚   â”œâ”€â”€ 04_transformer_block.ipynb
â”‚   â”œâ”€â”€ 05_model.ipynb
â”‚   â”œâ”€â”€ 06_training.ipynb
â”‚   â”œâ”€â”€ 07_generation.ipynb
â”‚   â”œâ”€â”€ 08_evaluation.ipynb
â”‚   â”œâ”€â”€ 09_fine_tuning.ipynb
â”‚   â”œâ”€â”€ data_preparation.ipynb
â”‚   â”œâ”€â”€ model_analysis.ipynb
â”‚   â””â”€â”€ deployment.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py          # Tokenizer utilities
â”‚   â”œâ”€â”€ model.py              # Model architecture
â”‚   â”œâ”€â”€ training.py           # Training utilities
â”‚   â”œâ”€â”€ generation.py         # Text generation functions
â”‚   â””â”€â”€ utils.py              # Helper functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw datasets
â”‚   â”œâ”€â”€ processed/            # Preprocessed data
â”‚   â””â”€â”€ tokenizers/           # Saved tokenizer models
â”œâ”€â”€ checkpoints/              # Model checkpoints
â”œâ”€â”€ results/                  # Training logs, metrics
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md       # Detailed architecture documentation
    â”œâ”€â”€ requirements.md       # Detailed requirements specification
    â””â”€â”€ testing.md            # Test plans and results
```

## ğŸ”® Future Extensions

- [ ] Multimodal inputs (speech, images) - Qwen-style
- [ ] LoRA/parameter-efficient fine-tuning optimization
- [ ] Knowledge distillation from larger models
- [ ] Synthetic dataset generation
- [ ] Chatbot API deployment
- [ ] Web application interface

## ğŸ“– References

- **NanoGPT**: [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
- **Attention is All You Need**: [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)
- **GPT-2**: [Radford et al., 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## ğŸ¤ Contributing

This project is managed via GitHub with team collaboration:
- **Repository**: Managed by Anthony Mauro
- **Team**: Abhiram, Ananya, Anthony, Raunak



## ğŸ™ Acknowledgments

- NSF-funded Nautilus NRP clusters for computational resources
- Open-source community (PyTorch, Hugging Face)

---

**Acronyms**:
- **LLM**: Large Language Model
- **NLP**: Natural Language Processing
- **HPC**: High Performance Computing
- **GPU**: Graphics Processing Unit
- **Q&A**: Question and Answer
- **NRP**: National Research Platform
