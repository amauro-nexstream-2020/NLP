apiVersion: batch/v1
kind: Job
metadata:
  name: pure-transformer-pretrain-debug
  namespace: ucsdfutures
  labels:
    app: pure-transformer
    phase: pretrain-debug
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: pure-transformer
        phase: pretrain-debug
    spec:
      restartPolicy: Never
      imagePullSecrets:
        - name: registry-credentials
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: trainer
          image: nvcr.io/nvidia/pytorch:24.01-py3
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euxo pipefail
              echo "== debug job: ensure workspace dir =="
              mkdir -p /workspace
              cd /workspace
              rm -rf nlp_project || true
              echo "== debug job: cloning repo (branch mamba-hybrid) =="
              git clone -b mamba-hybrid https://github.com/amauro-nexstream-2020/NLP.git nlp_project
              cd nlp_project
              echo "== debug job: verify torch and CUDA availability =="
              python -c "import torch; print('torch:', torch.__version__); print('cuda:', torch.version.cuda); import sys; print(sys.executable)"
              nvidia-smi || true

              echo "== debug job: ensure build tools (apt) =="
              # Install common build tools required for building some Python extensions (cmake, ninja, g++)
              apt-get update -y && apt-get install -y --no-install-recommends \
                build-essential cmake ninja-build git python3-dev libssl-dev libffi-dev pkg-config || true

              echo "== debug job: upgrade pip, wheel, setuptools =="
              python -m pip install --upgrade pip setuptools wheel

              echo "== debug job: installing python deps (without torch) =="
              # Do not install torch / cuda packages; base image provides those
              if [ -f requirements.txt ]; then
                grep -v -iE '^torch' requirements.txt > /tmp/reqs_no_torch.txt || true
                python -m pip install --no-cache-dir -r /tmp/reqs_no_torch.txt
              fi
              python -m pip install --no-cache-dir --prefer-binary tokenizers sentencepiece datasets transformers clearml lightning pyyaml requests scikit-learn bitsandbytes
              # Install torchvision without dependency resolution to avoid pip re-installing torch
              python -m pip install --no-cache-dir --no-deps torchvision
              echo "== debug job: install flash-attn & mamba-ssm using system torch =="
              echo "== debug job: install flash-attn & mamba-ssm using system torch =="
              # Prefer prebuilt binary wheel first; if not available, build using system torch
              python -m pip install --no-cache-dir --prefer-binary flash-attn==2.5.6 || python -m pip install --no-cache-dir --no-build-isolation flash-attn==2.5.6
              python -m pip install --no-cache-dir --prefer-binary mamba-ssm==2.2.6.post3 || python -m pip install --no-cache-dir --no-build-isolation mamba-ssm==2.2.6.post3
              echo "== debug job: sanity checks (pip check, freeze) =="
              python -m pip check || true
              python -m pip freeze | sed -n '1,200p'
              echo "== debug job: verifying deps =="
              python -c "import torch; print('torch', torch.__version__, torch.cuda.is_available())"
              python -c "import lightning; import inspect; print('lightning', lightning.__version__, 'file', inspect.getsourcefile(lightning))"
              echo "== debug job: running pretrain (debug preset) =="
              export PYTHONPATH=/workspace/nlp_project:$PYTHONPATH
              python -u -m pure_transformer.run_pretrain --config debug --model tiny --checkpoint-dir /checkpoints
          resources:
            requests:
              memory: "32Gi"
              cpu: "4"
              nvidia.com/a100: "1"
            limits:
              memory: "64Gi"
              cpu: "8"
              nvidia.com/a100: "1"
          env:
            # ClearML credentials (optional - put in secret)
            - name: CLEARML_WEB_HOST
              valueFrom:
                secretKeyRef:
                  name: pure-transformer-secrets
                  key: clearml-web-host
                  optional: true
            - name: CLEARML_API_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: pure-transformer-secrets
                  key: clearml-api-access-key
                  optional: true
            - name: CLEARML_API_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: pure-transformer-secrets
                  key: clearml-api-secret-key
                  optional: true
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: pure-transformer-secrets
                  key: hf-token
                  optional: true
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "max_split_size_mb:128"
            - name: PIP_CACHE_DIR
              value: "/root/.cache/pip"
            - name: XDG_CACHE_HOME
              value: "/root/.cache"
          volumeMounts:
            - name: checkpoints
              mountPath: /checkpoints
            - name: shm
              mountPath: /dev/shm
            - name: cache
              mountPath: /root/.cache
      volumes:
        - name: checkpoints
          persistentVolumeClaim:
            claimName: pure-transformer-checkpoints
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
        - name: cache
          emptyDir:
            sizeLimit: 20Gi
      imagePullSecrets:
        - name: registry-credentials
