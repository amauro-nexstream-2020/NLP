{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 Pure Transformer - Interactive Model Exploration\n",
        "\n",
        "This notebook provides interactive visualizations and analysis of the Pure Transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pure_transformer.model import TransformerLM, TransformerConfig\n",
        "from pure_transformer.configs import TINY_CONFIG, SMALL_CONFIG, MEDIUM_CONFIG\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model Instantiation\n",
        "\n",
        "Let's create a model and explore its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a tiny model for exploration\n",
        "model = TransformerLM(TINY_CONFIG)\n",
        "print(f\"Model created with {model.count_parameters():,} parameters\")\n",
        "print(f\"\\nModel configuration:\")\n",
        "print(TINY_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Parameter Distribution\n",
        "\n",
        "Visualize where parameters are distributed across the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Analyze parameter distribution\n",
        "param_counts = {}\n",
        "for name, module in model.named_modules():\n",
        "    if len(list(module.children())) == 0:  # Leaf module\n",
        "        params = sum(p.numel() for p in module.parameters())\n",
        "        if params > 0:\n",
        "            module_type = module.__class__.__name__\n",
        "            param_counts[module_type] = param_counts.get(module_type, 0) + params\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(param_counts)))\n",
        "plt.bar(param_counts.keys(), param_counts.values(), color=colors)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Number of Parameters')\n",
        "plt.title('Parameter Distribution by Module Type')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nParameter breakdown:\")\n",
        "for module_type, count in sorted(param_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{module_type:30s}: {count:12,} ({100*count/model.count_parameters():.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Attention Pattern Visualization\n",
        "\n",
        "Visualize attention patterns for a sample input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create sample input\n",
        "sample_input = torch.randint(0, 1000, (1, 32))\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    logits = model(sample_input)\n",
        "\n",
        "print(f\"Input shape: {sample_input.shape}\")\n",
        "print(f\"Output shape: {logits.shape}\")\n",
        "print(f\"\\nTop 5 predicted tokens: {logits[0, -1].topk(5).indices.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Size Comparison\n",
        "\n",
        "Compare different model configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "configs = {\n",
        "    'TINY': TINY_CONFIG,\n",
        "    'SMALL': SMALL_CONFIG,\n",
        "    'MEDIUM': MEDIUM_CONFIG,\n",
        "}\n",
        "\n",
        "comparison = {}\n",
        "for name, config in configs.items():\n",
        "    temp_model = TransformerLM(config)\n",
        "    comparison[name] = {\n",
        "        'Parameters': temp_model.count_parameters(),\n",
        "        'Layers': config.num_layers,\n",
        "        'Hidden Size': config.hidden_size,\n",
        "        'Heads': config.num_heads,\n",
        "    }\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Model Configuration Comparison', fontsize=16)\n",
        "\n",
        "for idx, (metric, ax) in enumerate(zip(['Parameters', 'Layers', 'Hidden Size', 'Heads'], axes.flat)):\n",
        "    values = [comparison[name][metric] for name in configs.keys()]\n",
        "    ax.bar(configs.keys(), values, color=['#4CAF50', '#2196F3', '#FF9800'])\n",
        "    ax.set_title(metric)\n",
        "    ax.set_ylabel('Value')\n",
        "    if metric == 'Parameters':\n",
        "        ax.set_ylabel('Parameters (Millions)')\n",
        "        ax.set_yticklabels([f'{int(y/1e6)}M' for y in ax.get_yticks()])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Metrics Simulation\n",
        "\n",
        "Simulate and visualize training metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Simulate training curve\n",
        "steps = np.arange(0, 10000, 100)\n",
        "warmup_steps = 1000\n",
        "max_lr = 3e-4\n",
        "min_lr = 3e-5\n",
        "\n",
        "def cosine_schedule(step, warmup_steps, total_steps, max_lr, min_lr):\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * step / warmup_steps\n",
        "    progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "lrs = [cosine_schedule(s, warmup_steps, 10000, max_lr, min_lr) for s in steps]\n",
        "\n",
        "# Plot learning rate schedule\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(steps, lrs, linewidth=2, color='#667eea')\n",
        "plt.axvline(warmup_steps, color='red', linestyle='--', alpha=0.5, label='End of Warmup')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Learning Rate Schedule (Cosine with Warmup)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Simulate loss curve\n",
        "plt.subplot(1, 2, 2)\n",
        "loss = 8 - 5 * (1 - np.exp(-steps/2000)) + np.random.normal(0, 0.1, len(steps))\n",
        "plt.plot(steps, loss, linewidth=2, color='#764ba2', alpha=0.7)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Component Interaction Map\n",
        "\n",
        "Visualize how different components interact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "print(\"Component Dependencies:\")\n",
        "print(\"\\nTransformerLM depends on:\")\n",
        "print(\"  \u251c\u2500 Token Embeddings\")\n",
        "print(\"  \u251c\u2500 RoPE Cache (precompute_rope_cache)\")\n",
        "print(\"  \u251c\u2500 Transformer Blocks (24\u00d7)\")\n",
        "print(\"  \u2502   \u251c\u2500 RMSNorm\")\n",
        "print(\"  \u2502   \u251c\u2500 Attention\")\n",
        "print(\"  \u2502   \u2502   \u251c\u2500 Q/K/V Projections\")\n",
        "print(\"  \u2502   \u2502   \u251c\u2500 apply_rotary_emb\")\n",
        "print(\"  \u2502   \u2502   \u251c\u2500 Flash Attention / SDPA\")\n",
        "print(\"  \u2502   \u2502   \u2514\u2500 Output Projection\")\n",
        "print(\"  \u2502   \u251c\u2500 RMSNorm\")\n",
        "print(\"  \u2502   \u2514\u2500 SwiGLUMLP\")\n",
        "print(\"  \u2502       \u251c\u2500 Gate Projection\")\n",
        "print(\"  \u2502       \u251c\u2500 Up Projection\")\n",
        "print(\"  \u2502       \u2514\u2500 Down Projection\")\n",
        "print(\"  \u251c\u2500 Final RMSNorm\")\n",
        "print(\"  \u2514\u2500 LM Head\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}