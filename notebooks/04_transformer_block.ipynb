{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. The Transformer Block\n",
    "\n",
    "In the previous notebook, we implemented the Self-Attention mechanism, which allows the model to attend to different parts of the input sequence. However, attention alone is not enough. To build a powerful language model, we need to combine attention with other components: **Layer Normalization**, **Feed-Forward Networks**, and **Residual Connections**.\n",
    "\n",
    "These components come together to form the **Transformer Block**, the fundamental building block of the Transformer architecture.\n",
    "\n",
    "## Goals\n",
    "1.  Understand and implement **Layer Normalization**.\n",
    "2.  Understand and implement the **Feed-Forward Network (FFN)**.\n",
    "3.  Understand **Residual Connections**.\n",
    "4.  Assemble the complete **Transformer Decoder Block**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configuration class to hold model hyperparameters\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_positions: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer Normalization\n",
    "\n",
    "Layer Normalization (LayerNorm) is a technique used to stabilize the training of deep neural networks. It normalizes the inputs across the features dimension (instead of the batch dimension like Batch Normalization).\n",
    "\n",
    "For a given input vector $x$, LayerNorm computes:\n",
    "\n",
    "$$ \\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta $$\n",
    "\n",
    "where:\n",
    "- $\\mu$ is the mean of the elements in $x$.\n",
    "- $\\sigma$ is the standard deviation.\n",
    "- $\\epsilon$ is a small constant for numerical stability.\n",
    "- $\\gamma$ (scale) and $\\beta$ (shift) are learnable parameters.\n",
    "\n",
    "In PyTorch, we can use `nn.LayerNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 4])\n",
      "Output shape: torch.Size([2, 3, 4])\n",
      "Mean of output (approx 0): tensor([[-4.4703e-08, -2.9802e-08, -2.6077e-08],\n",
      "        [-5.9605e-08,  5.9605e-08,  1.4901e-08]], grad_fn=<MeanBackward1>)\n",
      "Std of output (approx 1): tensor([[1.1547, 1.1547, 1.1547],\n",
      "        [1.1547, 1.1547, 1.1547]], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example of LayerNorm\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "n_embd = 4\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "ln = nn.LayerNorm(n_embd)\n",
    "out = ln(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Mean of output (approx 0):\", out.mean(dim=-1))\n",
    "print(\"Std of output (approx 1):\", out.std(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network (FFN)\n",
    "\n",
    "The Feed-Forward Network is a simple Multi-Layer Perceptron (MLP) applied independently to each position in the sequence. It consists of two linear transformations with a non-linear activation function in between.\n",
    "\n",
    "The standard architecture expands the dimensionality by a factor of 4 and then projects it back.\n",
    "\n",
    "$$ \\text{FFN}(x) = \\text{GELU}(x W_1 + b_1) W_2 + b_2 $$\n",
    "\n",
    "We use the **GELU** (Gaussian Error Linear Unit) activation function, which is commonly used in Transformers (like GPT-2 and BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        # Inner dimension is usually 4 * n_embd\n",
    "        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [batch, seq_len, n_embd]\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention (Recap)\n",
    "\n",
    "For completeness, we include the `MultiHeadAttention` class from the previous notebook here so we can build the full block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer with causal masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # QKV projection\n",
    "        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Dropout\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "            .view(1, 1, config.n_positions, config.n_positions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Calculate QKV\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # [B, n_head, T, head_dim]\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        y = attn @ v  # [B, n_head, T, head_dim]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Transformer Block\n",
    "\n",
    "Now we assemble the components. A standard Transformer Decoder Block consists of:\n",
    "\n",
    "1.  **LayerNorm** (before attention)\n",
    "2.  **Multi-Head Attention**\n",
    "3.  **Residual Connection** (add attention output to input)\n",
    "4.  **LayerNorm** (before FFN)\n",
    "5.  **Feed-Forward Network**\n",
    "6.  **Residual Connection** (add FFN output to input)\n",
    "\n",
    "This \"Pre-Norm\" architecture (LayerNorm before the sub-layers) is standard in modern Transformers like GPT-2 and GPT-3 as it improves training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.ffn = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-norm architecture (GPT-2 style)\n",
    "        # 1. Attention branch\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        \n",
    "        # 2. FFN branch\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification\n",
    "\n",
    "Let's verify that our block works as expected by passing some dummy data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: ModelConfig(n_embd=768, n_head=12, n_layer=12, n_positions=1024, vocab_size=50257, dropout=0.1, bias=True)\n",
      "Input shape: torch.Size([2, 32, 768])\n",
      "Output shape: torch.Size([2, 32, 768])\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig()\n",
    "block = TransformerBlock(config)\n",
    "\n",
    "print(\"Model Config:\", config)\n",
    "\n",
    "# Create dummy input: [batch_size, seq_len, n_embd]\n",
    "x = torch.randn(2, 32, config.n_embd)\n",
    "\n",
    "# Forward pass\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "assert output.shape == x.shape, \"Output shape must match input shape!\"\n",
    "print(\"Verification successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
