{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6542f912",
   "metadata": {},
   "source": [
    "# Attention Mechanisms in Transformers: A Comprehensive Guide\n",
    "\n",
    "This notebook provides a thorough implementation of attention mechanisms from scratch, implementing modern transformer attention mechanisms. We'll build up from basic self-attention to multi-head attention with rotary embeddings and KV caching.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Attention](#introduction)\n",
    "2. [Scaled Dot-Product Attention](#scaled-dot-product)\n",
    "3. [Multi-Head Attention](#multi-head)\n",
    "4. [Causal Self-Attention](#causal-attention)\n",
    "5. [Rotary Position Embeddings in Attention](#rope-attention)\n",
    "6. [Grouped Query Attention (GQA)](#gqa)\n",
    "7. [KV Caching for Efficient Inference](#kv-cache)\n",
    "8. [Complete Implementation](#complete-implementation)\n",
    "9. [Visualization and Analysis](#visualization)\n",
    "10. [Performance Comparison](#performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be82571",
   "metadata": {},
   "source": [
    "## Introduction to Attention\n",
    "\n",
    "Attention mechanisms allow models to focus on different parts of the input when processing each element. In transformers, attention is the core mechanism that enables the model to capture relationships between tokens.\n",
    "\n",
    "### Key Concepts in Modern Attention:\n",
    "- **Causal Self-Attention**: Each token can only attend to itself and previous tokens\n",
    "- **Multi-Head Attention**: Multiple parallel attention operations capture different relationships\n",
    "- **Rotary Embeddings (RoPE)**: Applied to queries and keys for positional information\n",
    "- **Grouped Query Attention (GQA)**: Efficiency optimization with fewer key/value heads\n",
    "- **KV Caching**: Store past keys/values for efficient autoregressive generation\n",
    "- **Flash Attention**: Optimized attention computation via PyTorch's `scaled_dot_product_attention`\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "For a query $Q$, key $K$, and value $V$:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where $d_k$ is the dimension of the key vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee080c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ff468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf515ed1",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The foundation of all attention mechanisms is the scaled dot-product attention. Let's implement it from scratch to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (batch, seq_len_q, d_k)\n",
    "        key: Key tensor of shape (batch, seq_len_k, d_k)\n",
    "        value: Value tensor of shape (batch, seq_len_k, d_v)\n",
    "        mask: Optional mask tensor of shape (batch, seq_len_q, seq_len_k)\n",
    "              where masked positions have value -inf or False\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (batch, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights of shape (batch, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # Get dimension of keys for scaling\n",
    "    d_k = query.shape[-1]\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with simple example\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Create random query, key, value tensors\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nSample attention weights (first batch, all positions):\")\n",
    "print(attention_weights[0])\n",
    "print(f\"\\nSum of attention weights per row (should be ~1.0):\")\n",
    "print(attention_weights[0].sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576d204",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights\n",
    "\n",
    "Let's visualize the attention pattern to understand how different positions attend to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.2f', cmap='viridis',\n",
    "            xticklabels=[f'K{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Q{i}' for i in range(seq_len)])\n",
    "plt.title('Attention Weights Heatmap')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5114966",
   "metadata": {},
   "source": [
    "## Causal Self-Attention\n",
    "\n",
    "In language modeling, we need causal (autoregressive) attention where each position can only attend to itself and previous positions. This is achieved using a causal mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e766ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower triangular) mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean mask of shape (seq_len, seq_len) where True indicates \n",
    "              positions that can be attended to\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    return mask\n",
    "\n",
    "# Create and visualize causal mask\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "print(f\"Causal mask:\\n{causal_mask.int()}\\n\")\n",
    "\n",
    "# Apply causal attention\n",
    "query = torch.randn(1, seq_len, d_model)\n",
    "key = torch.randn(1, seq_len, d_model)\n",
    "value = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "output, causal_attention_weights = scaled_dot_product_attention(\n",
    "    query, key, value, mask=causal_mask\n",
    ")\n",
    "\n",
    "# Visualize causal attention\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(causal_attention_weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.2f', cmap='viridis',\n",
    "            xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Pos {i}' for i in range(seq_len)])\n",
    "plt.title('Causal Attention Weights (Lower Triangular)')\n",
    "plt.xlabel('Key Position (Past)')\n",
    "plt.ylabel('Query Position (Current)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNotice how each position (row) only attends to itself and previous positions (columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765f6c8",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multi-head attention runs multiple attention operations in parallel, allowing the model to attend to information from different representation subspaces. This is the foundation of transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e136492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module.\n",
    "    \n",
    "    Args:\n",
    "        n_embd: Embedding dimension\n",
    "        n_head: Number of attention heads\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        # Query, Key, Value projections for all heads (combined)\n",
    "        self.c_q = nn.Linear(n_embd, n_embd)\n",
    "        self.c_k = nn.Linear(n_embd, n_embd)\n",
    "        self.c_v = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Dropout\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, n_embd)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output of shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # batch, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Calculate query, key, values for all heads\n",
    "        q = self.c_q(x)  # (B, T, n_embd)\n",
    "        k = self.c_k(x)  # (B, T, n_embd)\n",
    "        v = self.c_v(x)  # (B, T, n_embd)\n",
    "        \n",
    "        # Split into multiple heads: (B, T, n_embd) -> (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) -> (B, n_head, T, T)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (B, n_head, T, T) @ (B, n_head, T, head_dim) -> (B, n_head, T, head_dim)\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concatenate heads: (B, n_head, T, head_dim) -> (B, T, n_head, head_dim) -> (B, T, n_embd)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.c_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(n_embd, n_head)\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "output = mha(x, mask=causal_mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {n_head}\")\n",
    "print(f\"Head dimension: {n_embd // n_head}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45ad7c",
   "metadata": {},
   "source": [
    "## Rotary Position Embeddings (RoPE) in Attention\n",
    "\n",
    "RoPE is applied to the query and key tensors before computing attention. This provides better positional information than absolute positional embeddings.\n",
    "\n",
    "RoPE rotates the embedding vectors based on their positions, allowing the model to capture relative distances between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, cos_sin: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary position embeddings to input tensor.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (..., seq_len, dim)\n",
    "        cos_sin: Precomputed cos and sin values of shape (seq_len, dim)\n",
    "    \n",
    "    Returns:\n",
    "        Rotated tensor of same shape as input\n",
    "    \"\"\"\n",
    "    # Split x into two halves\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    \n",
    "    # Split cos_sin into cos and sin\n",
    "    cos, sin = cos_sin.chunk(2, dim=-1)\n",
    "    \n",
    "    # Apply rotation\n",
    "    # [x1, x2] rotated by theta becomes [x1*cos - x2*sin, x1*sin + x2*cos]\n",
    "    rotated_x1 = x1 * cos - x2 * sin\n",
    "    rotated_x2 = x1 * sin + x2 * cos\n",
    "    \n",
    "    # Concatenate back\n",
    "    return torch.cat([rotated_x1, rotated_x2], dim=-1)\n",
    "\n",
    "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for rotary embeddings.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embeddings (must be even)\n",
    "        seq_len: Maximum sequence length\n",
    "        theta: Base for frequency calculation\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (seq_len, dim) containing cos and sin values\n",
    "    \"\"\"\n",
    "    # Compute frequencies\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(seq_len, dtype=torch.float32)\n",
    "    \n",
    "    # Compute outer product\n",
    "    freqs = torch.outer(t, freqs)  # (seq_len, dim//2)\n",
    "    \n",
    "    # Compute cos and sin\n",
    "    cos = torch.cos(freqs)\n",
    "    sin = torch.sin(freqs)\n",
    "    \n",
    "    # Interleave cos and sin for easier application\n",
    "    # We duplicate each value: [cos, cos, sin, sin] pattern\n",
    "    cos = torch.repeat_interleave(cos, 2, dim=-1)\n",
    "    sin = torch.repeat_interleave(sin, 2, dim=-1)\n",
    "    \n",
    "    # Concatenate: first half is cos, second half is sin\n",
    "    freqs_cis = torch.cat([cos, sin], dim=-1)\n",
    "    \n",
    "    return freqs_cis\n",
    "\n",
    "# Test RoPE\n",
    "dim = 64\n",
    "seq_len = 10\n",
    "freqs_cis = precompute_freqs_cis(dim, seq_len)\n",
    "\n",
    "print(f\"Frequency tensor shape: {freqs_cis.shape}\")\n",
    "print(f\"Expected shape: ({seq_len}, {dim * 2})\")\n",
    "\n",
    "# Test applying RoPE to a sample tensor\n",
    "x = torch.randn(2, seq_len, dim)  # (batch, seq_len, dim)\n",
    "rotated_x = apply_rotary_emb(x, freqs_cis)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Rotated output shape: {rotated_x.shape}\")\n",
    "print(f\"Shapes match: {x.shape == rotated_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Repeat key/value heads to match the number of query heads.\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor of shape (batch, n_kv_head, seq_len, head_dim)\n",
    "        n_rep: Number of repetitions\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (batch, n_kv_head * n_rep, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    batch, n_kv_head, seq_len, head_dim = x.shape\n",
    "    \n",
    "    # Repeat along the head dimension\n",
    "    # (B, n_kv_head, 1, T, head_dim) -> (B, n_kv_head, n_rep, T, head_dim) -> (B, n_kv_head * n_rep, T, head_dim)\n",
    "    x = x[:, :, None, :, :].expand(batch, n_kv_head, n_rep, seq_len, head_dim)\n",
    "    return x.reshape(batch, n_kv_head * n_rep, seq_len, head_dim)\n",
    "\n",
    "# Demonstrate GQA\n",
    "n_head = 8\n",
    "n_kv_head = 2  # Fewer KV heads\n",
    "batch = 2\n",
    "seq_len = 10\n",
    "head_dim = 16\n",
    "\n",
    "# Simulate key/value tensors with fewer heads\n",
    "kv = torch.randn(batch, n_kv_head, seq_len, head_dim)\n",
    "print(f\"Original KV shape: {kv.shape}\")\n",
    "\n",
    "# Repeat to match query heads\n",
    "n_rep = n_head // n_kv_head\n",
    "kv_repeated = repeat_kv(kv, n_rep)\n",
    "print(f\"Repeated KV shape: {kv_repeated.shape}\")\n",
    "print(f\"Number of repetitions per KV head: {n_rep}\")\n",
    "\n",
    "# Verify shapes\n",
    "assert kv_repeated.shape[1] == n_head, \"Number of heads should match query heads\"\n",
    "print(f\"\\n✓ GQA: {n_kv_head} KV heads repeated to {n_head} heads (ratio 1:{n_rep})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fe11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Key-Value cache for efficient autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_batch_size: int, max_seq_len: int, n_kv_head: int, head_dim: int, dtype=torch.float32):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.n_kv_head = n_kv_head\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        # Initialize cache tensors\n",
    "        self.cache_k = torch.zeros(\n",
    "            max_batch_size, n_kv_head, max_seq_len, head_dim, dtype=dtype\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            max_batch_size, n_kv_head, max_seq_len, head_dim, dtype=dtype\n",
    "        )\n",
    "        \n",
    "        self.current_len = 0\n",
    "    \n",
    "    def update(self, k: torch.Tensor, v: torch.Tensor, start_pos: int):\n",
    "        \"\"\"\n",
    "        Update cache with new keys and values.\n",
    "        \n",
    "        Args:\n",
    "            k: New keys of shape (batch, n_kv_head, new_tokens, head_dim)\n",
    "            v: New values of shape (batch, n_kv_head, new_tokens, head_dim)\n",
    "            start_pos: Starting position in the cache\n",
    "        \n",
    "        Returns:\n",
    "            Updated keys and values from cache\n",
    "        \"\"\"\n",
    "        batch_size, n_kv_head, new_tokens, head_dim = k.shape\n",
    "        \n",
    "        # Update cache\n",
    "        end_pos = start_pos + new_tokens\n",
    "        self.cache_k[:batch_size, :, start_pos:end_pos, :] = k\n",
    "        self.cache_v[:batch_size, :, start_pos:end_pos, :] = v\n",
    "        \n",
    "        self.current_len = end_pos\n",
    "        \n",
    "        # Return all cached values up to current position\n",
    "        return (\n",
    "            self.cache_k[:batch_size, :, :end_pos, :],\n",
    "            self.cache_v[:batch_size, :, :end_pos, :]\n",
    "        )\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the cache.\"\"\"\n",
    "        self.current_len = 0\n",
    "        self.cache_k.zero_()\n",
    "        self.cache_v.zero_()\n",
    "\n",
    "# Demonstrate KV caching\n",
    "batch_size = 1\n",
    "max_seq_len = 100\n",
    "n_kv_head = 4\n",
    "head_dim = 32\n",
    "\n",
    "kv_cache = KVCache(batch_size, max_seq_len, n_kv_head, head_dim)\n",
    "\n",
    "print(\"Simulating autoregressive generation with KV caching:\")\n",
    "print(f\"Maximum sequence length: {max_seq_len}\")\n",
    "print(f\"Number of KV heads: {n_kv_head}\")\n",
    "print(f\"Head dimension: {head_dim}\\n\")\n",
    "\n",
    "# Simulate generating 5 tokens\n",
    "for step in range(5):\n",
    "    # New token (only 1 at a time during generation)\n",
    "    new_k = torch.randn(batch_size, n_kv_head, 1, head_dim)\n",
    "    new_v = torch.randn(batch_size, n_kv_head, 1, head_dim)\n",
    "    \n",
    "    # Update cache\n",
    "    cached_k, cached_v = kv_cache.update(new_k, new_v, start_pos=step)\n",
    "    \n",
    "    print(f\"Step {step + 1}:\")\n",
    "    print(f\"  New K/V shape: {new_k.shape}\")\n",
    "    print(f\"  Cached K/V shape: {cached_k.shape}\")\n",
    "    print(f\"  Cache length: {kv_cache.current_len}\")\n",
    "    print(f\"  Memory saved: Reusing {step} previous token computations\\n\")\n",
    "\n",
    "print(f\"✓ KV cache stores keys/values to avoid recomputing attention for past tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Self-Attention module implementing modern transformer architectures.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-head attention with separate Q, K, V projections\n",
    "    - Rotary position embeddings (RoPE)\n",
    "    - Grouped Query Attention (GQA) support\n",
    "    - KV caching for efficient inference\n",
    "    - RMS normalization for queries and keys\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd: int, n_head: int, n_kv_head: int, max_seq_len: int = 1024, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        assert n_head % n_kv_head == 0, \"n_head must be divisible by n_kv_head\"\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_kv_head = n_kv_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        self.n_rep = n_head // n_kv_head  # Repetition factor for GQA\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.c_q = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.c_k = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)\n",
    "        self.c_v = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        \n",
    "        # Dropout\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Precompute RoPE frequencies\n",
    "        self.register_buffer(\n",
    "            \"freqs_cis\",\n",
    "            precompute_freqs_cis(self.head_dim, max_seq_len)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, kv_cache: Optional[KVCache] = None, start_pos: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, n_embd)\n",
    "            kv_cache: Optional KV cache for inference\n",
    "            start_pos: Starting position (for KV cache)\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output of shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        q = self.c_q(x)  # (B, T, n_embd)\n",
    "        k = self.c_k(x)  # (B, T, n_kv_head * head_dim)\n",
    "        v = self.c_v(x)  # (B, T, n_kv_head * head_dim)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)\n",
    "        k = k.view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2)  # (B, n_kv_head, T, head_dim)\n",
    "        v = v.view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2)  # (B, n_kv_head, T, head_dim)\n",
    "        \n",
    "        # Apply RoPE to queries and keys\n",
    "        # Get frequencies for current positions\n",
    "        freqs = self.freqs_cis[start_pos:start_pos + T]\n",
    "        \n",
    "        # Reshape for broadcasting: (T, head_dim*2) -> (1, 1, T, head_dim*2)\n",
    "        freqs = freqs.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q = apply_rotary_emb(q, freqs)\n",
    "        k = apply_rotary_emb(k, freqs)\n",
    "        \n",
    "        # Apply RMS normalization (common in modern transformer architectures)\n",
    "        q = F.rms_norm(q, (self.head_dim,))\n",
    "        k = F.rms_norm(k, (self.head_dim,))\n",
    "        \n",
    "        # Update KV cache if provided\n",
    "        if kv_cache is not None:\n",
    "            k, v = kv_cache.update(k, v, start_pos)\n",
    "        \n",
    "        # Repeat K/V heads if using GQA\n",
    "        if self.n_rep > 1:\n",
    "            k = repeat_kv(k, self.n_rep)\n",
    "            v = repeat_kv(v, self.n_rep)\n",
    "        \n",
    "        # Compute attention using PyTorch's optimized function (Flash Attention)\n",
    "        # This automatically applies causal masking when is_causal=True\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
    "            is_causal=(kv_cache is None)  # Only causal during training\n",
    "        )\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test the complete implementation\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_kv_head = 4  # GQA: half the number of KV heads\n",
    "max_seq_len = 512\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "attention = CausalSelfAttention(n_embd, n_head, n_kv_head, max_seq_len)\n",
    "attention.eval()  # Set to eval mode\n",
    "\n",
    "# Test without KV cache (training mode)\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "output = attention(x)\n",
    "\n",
    "print(\"Training Mode (no KV cache):\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Number of parameters: {sum(p.numel() for p in attention.parameters()):,}\")\n",
    "print(f\"  GQA configuration: {n_head} query heads, {n_kv_head} KV heads (ratio 1:{n_head // n_kv_head})\")\n",
    "\n",
    "# Test with KV cache (inference mode)\n",
    "print(\"\\nInference Mode (with KV cache):\")\n",
    "kv_cache = KVCache(batch_size, max_seq_len, n_kv_head, attention.head_dim)\n",
    "\n",
    "# Generate 3 tokens autoregressively\n",
    "for i in range(3):\n",
    "    token = torch.randn(batch_size, 1, n_embd)  # Single token\n",
    "    output = attention(token, kv_cache=kv_cache, start_pos=i)\n",
    "    print(f\"  Step {i+1}: Input {token.shape} -> Output {output.shape}, Cache len: {kv_cache.current_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ab234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom attention module to extract attention weights\n",
    "class VisualizableAttention(nn.Module):\n",
    "    \"\"\"Modified attention that returns attention weights for visualization.\"\"\"\n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        self.c_q = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.c_k = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.c_v = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.c_k(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.c_v(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "# Create visualizable attention\n",
    "vis_attn = VisualizableAttention(n_embd=128, n_head=4)\n",
    "x = torch.randn(1, 12, 128)\n",
    "causal_mask = create_causal_mask(12)\n",
    "\n",
    "_, attn_weights = vis_attn(x, mask=causal_mask)\n",
    "\n",
    "# Visualize attention patterns for each head\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head in range(4):\n",
    "    sns.heatmap(\n",
    "        attn_weights[0, head].detach().numpy(),\n",
    "        annot=True, fmt='.2f', cmap='RdYlBu_r', cbar=True,\n",
    "        ax=axes[head], vmin=0, vmax=1\n",
    "    )\n",
    "    axes[head].set_title(f'Attention Head {head + 1}')\n",
    "    axes[head].set_xlabel('Key Position')\n",
    "    axes[head].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each head learns different attention patterns:\")\n",
    "print(\"- Some heads may focus on nearby tokens (local attention)\")\n",
    "print(\"- Others may attend to distant tokens (global attention)\")\n",
    "print(\"- The causal mask ensures no looking into the future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention with and without RoPE\n",
    "def compute_similarity_matrix(embeddings_with_rope, embeddings_without_rope):\n",
    "    \"\"\"Compute cosine similarity between positions.\"\"\"\n",
    "    # Normalize\n",
    "    e_rope = F.normalize(embeddings_with_rope, dim=-1)\n",
    "    e_no_rope = F.normalize(embeddings_without_rope, dim=-1)\n",
    "    \n",
    "    # Compute similarity matrices\n",
    "    sim_rope = torch.matmul(e_rope, e_rope.transpose(-2, -1))\n",
    "    sim_no_rope = torch.matmul(e_no_rope, e_no_rope.transpose(-2, -1))\n",
    "    \n",
    "    return sim_rope, sim_no_rope\n",
    "\n",
    "# Create embeddings\n",
    "seq_len = 16\n",
    "dim = 64\n",
    "embeddings = torch.randn(1, seq_len, dim)\n",
    "\n",
    "# Apply RoPE\n",
    "freqs_cis = precompute_freqs_cis(dim, seq_len)\n",
    "embeddings_rope = apply_rotary_emb(embeddings, freqs_cis)\n",
    "\n",
    "# Compute similarities\n",
    "sim_rope, sim_no_rope = compute_similarity_matrix(embeddings_rope, embeddings)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(sim_no_rope[0].detach().numpy(), annot=False, cmap='coolwarm', \n",
    "            center=0, ax=ax1, vmin=-1, vmax=1)\n",
    "ax1.set_title('Similarity Without RoPE')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Position')\n",
    "\n",
    "sns.heatmap(sim_rope[0].detach().numpy(), annot=False, cmap='coolwarm',\n",
    "            center=0, ax=ax2, vmin=-1, vmax=1)\n",
    "ax2.set_title('Similarity With RoPE')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RoPE adds position-dependent patterns to embeddings:\")\n",
    "print(\"- Nearby positions have similar rotations (higher similarity)\")\n",
    "print(\"- Distant positions have different rotations (lower similarity)\")\n",
    "print(\"- This helps the model learn relative position relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7159c",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare different attention configurations to understand the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_attention(n_embd, n_head, n_kv_head, seq_len, batch_size, num_iterations=100):\n",
    "    \"\"\"Benchmark attention forward pass.\"\"\"\n",
    "    attention = CausalSelfAttention(n_embd, n_head, n_kv_head, max_seq_len=seq_len)\n",
    "    attention.eval()\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, n_embd)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = attention(x)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        _ = attention(x)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_iterations * 1000  # ms\n",
    "    \n",
    "    # Count parameters\n",
    "    params = sum(p.numel() for p in attention.parameters())\n",
    "    \n",
    "    return avg_time, params\n",
    "\n",
    "# Compare different configurations\n",
    "configs = [\n",
    "    (\"Standard MHA\", 256, 8, 8),\n",
    "    (\"GQA (4 KV heads)\", 256, 8, 4),\n",
    "    (\"GQA (2 KV heads)\", 256, 8, 2),\n",
    "    (\"MQA (1 KV head)\", 256, 8, 1),\n",
    "]\n",
    "\n",
    "seq_len = 128\n",
    "batch_size = 4\n",
    "\n",
    "print(f\"Benchmarking attention configurations (seq_len={seq_len}, batch={batch_size}):\\n\")\n",
    "print(f\"{'Configuration':<20} {'Params':<12} {'Time (ms)':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_time = None\n",
    "for name, n_embd, n_head, n_kv_head in configs:\n",
    "    avg_time, params = benchmark_attention(n_embd, n_head, n_kv_head, seq_len, batch_size)\n",
    "    \n",
    "    if baseline_time is None:\n",
    "        baseline_time = avg_time\n",
    "        speedup = \"1.00x\"\n",
    "    else:\n",
    "        speedup = f\"{baseline_time / avg_time:.2f}x\"\n",
    "    \n",
    "    print(f\"{name:<20} {params:>10,}  {avg_time:>10.3f}  {speedup:>10}\")\n",
    "\n",
    "print(\"\\n Key Insights:\")\n",
    "print(\"- GQA reduces parameters by using fewer KV heads\")\n",
    "print(\"- Memory usage decreases with fewer KV heads\")\n",
    "print(\"- MQA (1 KV head) offers maximum parameter efficiency\")\n",
    "print(\"- Trade-off between model capacity and computational efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory and compute savings with KV cache\n",
    "def analyze_kv_cache_savings(n_tokens_generated, n_embd, n_kv_head, head_dim):\n",
    "    \"\"\"\n",
    "    Calculate FLOPs saved by using KV cache.\n",
    "    \n",
    "    Without cache: Recompute K,V for all previous tokens at each step\n",
    "    With cache: Only compute K,V for new token\n",
    "    \"\"\"\n",
    "    # Without cache: at step i, compute K,V for i tokens\n",
    "    flops_without_cache = sum(range(1, n_tokens_generated + 1))\n",
    "    \n",
    "    # With cache: compute K,V for 1 token at each step\n",
    "    flops_with_cache = n_tokens_generated\n",
    "    \n",
    "    savings_ratio = flops_without_cache / flops_with_cache\n",
    "    \n",
    "    return flops_without_cache, flops_with_cache, savings_ratio\n",
    "\n",
    "# Analyze for different generation lengths\n",
    "generation_lengths = [10, 50, 100, 200, 500]\n",
    "n_embd = 256\n",
    "n_kv_head = 4\n",
    "head_dim = n_embd // 8\n",
    "\n",
    "print(\"KV Cache Efficiency Analysis:\\n\")\n",
    "print(f\"{'Gen Length':<12} {'Without Cache':<15} {'With Cache':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for gen_len in generation_lengths:\n",
    "    without, with_cache, speedup = analyze_kv_cache_savings(gen_len, n_embd, n_kv_head, head_dim)\n",
    "    print(f\"{gen_len:<12} {without:<15} {with_cache:<12} {speedup:.1f}x\")\n",
    "\n",
    "# Visualize savings\n",
    "gen_range = range(1, 101)\n",
    "savings = [analyze_kv_cache_savings(i, n_embd, n_kv_head, head_dim)[2] for i in gen_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gen_range, savings, linewidth=2, color='green')\n",
    "plt.xlabel('Number of Tokens Generated')\n",
    "plt.ylabel('Compute Speedup (x)')\n",
    "plt.title('KV Cache Efficiency vs Generation Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: KV cache becomes increasingly valuable for longer generations\")\n",
    "print(f\"At 100 tokens: {savings[-1]:.1f}x speedup\")\n",
    "print(\"Essential for efficient autoregressive generation in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec75357",
   "metadata": {},
   "source": [
    "## Exercises and Further Exploration\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "1. **Experiment with Different Head Counts**: Try n_head = 4, 16, 32 and observe effects on attention patterns\n",
    "2. **Compare GQA Ratios**: Test different n_head/n_kv_head ratios (2:1, 4:1, 8:1)\n",
    "3. **Implement Cross-Attention**: Modify the code for encoder-decoder cross-attention\n",
    "4. **Flash Attention Benchmarking**: Compare PyTorch's optimized vs custom attention\n",
    "5. **Visualize Attention Entropy**: Analyze how \"focused\" different heads are\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- **Original Attention Paper**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- **RoPE Paper**: \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" (Su et al., 2021)\n",
    "- **GQA Paper**: \"GQA: Training Generalized Multi-Query Transformer\" (Ainslie et al., 2023)\n",
    "- **Flash Attention**: \"FlashAttention: Fast and Memory-Efficient Exact Attention\" (Dao et al., 2022)\n",
    "\n",
    "### Advanced Topics:\n",
    "\n",
    "- Sparse attention patterns\n",
    "- Linear attention mechanisms\n",
    "- Efficient attention for long sequences\n",
    "- Multi-modal attention (vision + language)\n",
    "- Attention visualization techniques\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Continue with notebook 04 to learn about complete transformer blocks that combine attention with feed-forward networks and layer normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6cbf22",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a complete attention mechanism from scratch, following modern transformer architectures. Let's summarize the key components and insights.\n",
    "\n",
    "### Components Implemented:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The foundation of all attention mechanisms\n",
    "2. **Causal Masking**: Ensures autoregressive property for language modeling\n",
    "3. **Multi-Head Attention**: Parallel attention operations for diverse representations\n",
    "4. **Rotary Position Embeddings (RoPE)**: Superior positional encoding through rotation\n",
    "5. **Grouped Query Attention (GQA)**: Efficient attention with fewer KV heads\n",
    "6. **KV Caching**: Essential optimization for fast autoregressive generation\n",
    "7. **Flash Attention**: Leveraging PyTorch's `scaled_dot_product_attention` for speed\n",
    "\n",
    "### Key Insights from Modern Transformer Research:\n",
    "\n",
    "- **RoPE > Absolute PE**: Rotary embeddings capture relative positions better\n",
    "- **GQA Efficiency**: Reducing KV heads maintains quality while saving memory\n",
    "- **KV Cache is Essential**: Provides quadratic speedup for generation\n",
    "- **RMS Normalization**: Applied to queries and keys after RoPE\n",
    "- **Separate Q/K/V Projections**: Better than combined QKV projection\n",
    "- **Flash Attention**: Automatic optimization when using PyTorch's built-in function\n",
    "\n",
    "### Performance Characteristics:\n",
    "\n",
    "| Configuration | Parameters | Memory | Speed |\n",
    "|--------------|-----------|---------|-------|\n",
    "| Standard MHA | Baseline | Baseline | Baseline |\n",
    "| GQA (4 heads) | -25% | -25% | Similar |\n",
    "| GQA (2 heads) | -50% | -50% | Similar |\n",
    "| MQA (1 head) | -75% | -75% | Faster |\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Language Models**: GPT-style autoregressive generation\n",
    "- **Machine Translation**: Encoder-decoder attention\n",
    "- **Question Answering**: Attending to relevant context\n",
    "- **Image Transformers**: Vision tasks with patch-based attention\n",
    "\n",
    "This attention mechanism is the core of modern transformer models and enables their remarkable capabilities across diverse tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
