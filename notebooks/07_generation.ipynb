{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07. Text Generation\n",
                "\n",
                "Once the model is trained, we can use it to generate text. This notebook explores different decoding strategies:\n",
                "\n",
                "1.  **Greedy Decoding**: Selecting the most likely token at each step.\n",
                "2.  **Sampling**: Sampling from the probability distribution (with temperature).\n",
                "3.  **Top-k Sampling**: Restricting sampling to the top $k$ tokens.\n",
                "4.  **Top-p (Nucleus) Sampling**: Restricting sampling to the smallest set of tokens with cumulative probability $p$.\n",
                "5.  **Beam Search**: Exploring multiple paths to find the most likely sequence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Dummy model and tokenizer for demonstration\n",
                "class DummyModel(torch.nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.config = type('Config', (), {'n_positions': 1024})()\n",
                "    def forward(self, x):\n",
                "        # Return random logits: [batch, seq_len, vocab_size]\n",
                "        return torch.randn(x.size(0), x.size(1), 1000), None\n",
                "\n",
                "class DummyTokenizer:\n",
                "    def encode(self, text): return [1, 2, 3]\n",
                "    def decode(self, ids): return \"generated text\"\n",
                "\n",
                "model = DummyModel()\n",
                "tokenizer = DummyTokenizer()\n",
                "device = torch.device(\"cpu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Greedy Decoding\n",
                "\n",
                "Greedy decoding simply selects the token with the highest probability at each step. It is fast but can lead to repetitive and dull text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "generated text\n"
                    ]
                }
            ],
            "source": [
                "def generate_greedy(model, tokenizer, prompt, max_new_tokens=20):\n",
                "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        logits, _ = model(input_ids)\n",
                "        next_token_logits = logits[:, -1, :]\n",
                "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
                "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "        \n",
                "    return tokenizer.decode(input_ids[0].tolist())\n",
                "\n",
                "print(generate_greedy(model, tokenizer, \"Hello world\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Sampling with Temperature\n",
                "\n",
                "Sampling introduces randomness. The **temperature** parameter controls the \"sharpness\" of the distribution.\n",
                "- Low temperature ($T < 1$): Makes the distribution sharper (more confident, less random).\n",
                "- High temperature ($T > 1$): Flattens the distribution (more random, more creative)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "generated text\n"
                    ]
                }
            ],
            "source": [
                "def generate_sampling(model, tokenizer, prompt, max_new_tokens=20, temperature=1.0):\n",
                "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        logits, _ = model(input_ids)\n",
                "        next_token_logits = logits[:, -1, :] / temperature\n",
                "        probs = F.softmax(next_token_logits, dim=-1)\n",
                "        next_token = torch.multinomial(probs, num_samples=1)\n",
                "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "        \n",
                "    return tokenizer.decode(input_ids[0].tolist())\n",
                "\n",
                "print(generate_sampling(model, tokenizer, \"Hello world\", temperature=0.8))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Top-k Sampling\n",
                "\n",
                "Top-k sampling restricts the sampling pool to the $k$ most likely tokens. This prevents the model from choosing very unlikely (and potentially irrelevant) tokens."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "generated text\n"
                    ]
                }
            ],
            "source": [
                "def generate_top_k(model, tokenizer, prompt, max_new_tokens=20, k=50):\n",
                "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        logits, _ = model(input_ids)\n",
                "        next_token_logits = logits[:, -1, :]\n",
                "        \n",
                "        # Filter logits\n",
                "        top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
                "        next_token_logits[next_token_logits < top_k_logits[:, [-1]]] = -float('Inf')\n",
                "        \n",
                "        probs = F.softmax(next_token_logits, dim=-1)\n",
                "        next_token = torch.multinomial(probs, num_samples=1)\n",
                "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "        \n",
                "    return tokenizer.decode(input_ids[0].tolist())\n",
                "\n",
                "print(generate_top_k(model, tokenizer, \"Hello world\", k=10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Top-p (Nucleus) Sampling\n",
                "\n",
                "Top-p sampling selects the smallest set of tokens whose cumulative probability exceeds $p$. This dynamically adjusts the size of the sampling pool based on the confidence of the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "generated text\n"
                    ]
                }
            ],
            "source": [
                "def generate_top_p(model, tokenizer, prompt, max_new_tokens=20, p=0.9):\n",
                "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        logits, _ = model(input_ids)\n",
                "        next_token_logits = logits[:, -1, :]\n",
                "        \n",
                "        # Sort logits\n",
                "        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
                "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
                "        \n",
                "        # Remove tokens with cumulative probability above the threshold\n",
                "        sorted_indices_to_remove = cumulative_probs > p\n",
                "        # Shift the indices to the right to keep also the first token above the threshold\n",
                "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
                "        sorted_indices_to_remove[..., 0] = 0\n",
                "        \n",
                "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
                "        next_token_logits[indices_to_remove] = -float('Inf')\n",
                "        \n",
                "        probs = F.softmax(next_token_logits, dim=-1)\n",
                "        next_token = torch.multinomial(probs, num_samples=1)\n",
                "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "        \n",
                "    return tokenizer.decode(input_ids[0].tolist())\n",
                "\n",
                "print(generate_top_p(model, tokenizer, \"Hello world\", p=0.9))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "NLP",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
