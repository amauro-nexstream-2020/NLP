{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Training the Model\n",
    "\n",
    "Now that we have our model architecture defined, we need to train it. This notebook covers the training pipeline, including:\n",
    "\n",
    "1.  **Data Loading**: Creating a PyTorch `DataLoader`.\n",
    "2.  **Optimization**: Setting up the optimizer (AdamW) and learning rate scheduler.\n",
    "3.  **Training Loop**: The core loop that updates model weights.\n",
    "4.  **Checkpointing**: Saving and loading model states.\n",
    "\n",
    "We will use a synthetic dataset for demonstration purposes, but the pipeline is identical for real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d85197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported DecoderLM from src.model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Try to import the actual model from src\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "try:\n",
    "    from src.model import DecoderLM, ModelConfig\n",
    "    print(\"Successfully imported DecoderLM from src.model\")\n",
    "except ImportError:\n",
    "    print(\"src.model not found. Using dummy class for demonstration.\")\n",
    "    from dataclasses import dataclass\n",
    "    @dataclass\n",
    "    class ModelConfig:\n",
    "        n_embd: int = 128\n",
    "        n_head: int = 4\n",
    "        n_layer: int = 2\n",
    "        n_positions: int = 128\n",
    "        vocab_size: int = 1000\n",
    "        dropout: float = 0.1\n",
    "        bias: bool = True\n",
    "\n",
    "    class DecoderLM(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            # Define parameters so the optimizer has something to optimize\n",
    "            self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "            self.config = config\n",
    "            \n",
    "        def forward(self, x, targets=None):\n",
    "            emb = self.token_emb(x)\n",
    "            logits = self.lm_head(emb)\n",
    "            loss = None\n",
    "            if targets is not None:\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce71210",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "We need a `Dataset` class that returns pairs of `(input_ids, target_ids)`. For causal language modeling, the target is usually the input shifted by one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cb797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input: tokens [i, i+seq_len]\n",
    "        # Target: tokens [i+1, i+seq_len+1]\n",
    "        chunk = self.data[idx : idx + self.seq_len + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return {\"input_ids\": x, \"labels\": y}\n",
    "\n",
    "# Create dummy data\n",
    "vocab_size = 1000\n",
    "data_size = 10000\n",
    "seq_len = 32\n",
    "dummy_data = torch.randint(0, vocab_size, (data_size,))\n",
    "\n",
    "dataset = TextDataset(dummy_data, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a8d65",
   "metadata": {},
   "source": [
    "## 2. Optimization\n",
    "\n",
    "We use the **AdamW** optimizer, which is standard for Transformers. We also use a learning rate scheduler with a warmup period followed by cosine decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bd3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DecoderLM(ModelConfig()).to(device)\n",
    "\n",
    "# Now this should work because model has parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Simple scheduler for demonstration (StepLR)\n",
    "# In production, use CosineAnnealingLR with warmup\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "\n",
    "Here is a basic training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c65443141f4bd6bb3009dfe8cbbdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 6.6549\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(input_ids, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Run one epoch\n",
    "avg_loss = train_epoch(model, dataloader, optimizer, device)\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpointing\n",
    "\n",
    "It's crucial to save your model periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "save_checkpoint(model, optimizer, 0, \"checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
