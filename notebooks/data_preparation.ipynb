{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preparation with Streaming\n",
                "\n",
                "Training Large Language Models requires massive datasets. Downloading terabytes of data to a local disk is often impractical. Instead, we can **stream** the data directly from the Hugging Face Hub.\n",
                "\n",
                "In this notebook, we will:\n",
                "1.  Stream the **FineWeb** dataset (or a sample of it) from Hugging Face.\n",
                "2.  Tokenize the text on-the-fly.\n",
                "3.  Create an `IterableDataset` for PyTorch training.\n",
                "4.  Save a small sample for local debugging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch.utils.data import IterableDataset, DataLoader\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer # Or our custom tokenizer\n",
                "\n",
                "# Use a popular tokenizer for demonstration (e.g., GPT-2)\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Streaming the Dataset\n",
                "\n",
                "We use the `datasets` library with `streaming=True`. This allows us to iterate over the dataset without downloading it.\n",
                "\n",
                "We'll use `HuggingFaceFW/fineweb-edu` (sample-10BT) as it is a high-quality web dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_name = \"HuggingFaceFW/fineweb-edu\"\n",
                "subset = \"sample-10BT\" # A smaller subset for demonstration\n",
                "\n",
                "print(f\"Streaming {dataset_name} ({subset})...\")\n",
                "dataset = load_dataset(dataset_name, name=subset, split=\"train\", streaming=True)\n",
                "\n",
                "# Peek at the first example\n",
                "print(next(iter(dataset)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. On-the-fly Tokenization\n",
                "\n",
                "We define a generator function that yields tokenized chunks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenization_generator(dataset, tokenizer, seq_len=1024):\n",
                "    buffer = []\n",
                "    for sample in dataset:\n",
                "        text = sample['text']\n",
                "        tokens = tokenizer.encode(text)\n",
                "        buffer.extend(tokens)\n",
                "        \n",
                "        # Yield chunks of seq_len + 1 (input + target)\n",
                "        while len(buffer) >= seq_len + 1:\n",
                "            yield torch.tensor(buffer[:seq_len + 1])\n",
                "            buffer = buffer[seq_len + 1:]\n",
                "\n",
                "# Create an IterableDataset wrapper\n",
                "class StreamedTextDataset(IterableDataset):\n",
                "    def __init__(self, dataset, tokenizer, seq_len):\n",
                "        self.dataset = dataset\n",
                "        self.tokenizer = tokenizer\n",
                "        self.seq_len = seq_len\n",
                "\n",
                "    def __iter__(self):\n",
                "        return tokenization_generator(self.dataset, self.tokenizer, self.seq_len)\n",
                "\n",
                "streamed_dataset = StreamedTextDataset(dataset, tokenizer, seq_len=128)\n",
                "dataloader = DataLoader(streamed_dataset, batch_size=4)\n",
                "\n",
                "# Test the dataloader\n",
                "batch = next(iter(dataloader))\n",
                "print(\"Batch shape:\", batch.shape) # Should be [4, 129]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Saving a Local Sample\n",
                "\n",
                "For debugging other notebooks without internet access or for faster iteration, it's useful to save a small chunk locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "local_samples = []\n",
                "for i, sample in enumerate(dataset):\n",
                "    if i >= 100: break\n",
                "    local_samples.append(sample['text'])\n",
                "\n",
                "with open(\"local_sample.json\", \"w\") as f:\n",
                "    json.dump(local_samples, f)\n",
                "    \n",
                "print(f\"Saved {len(local_samples)} samples to local_sample.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}