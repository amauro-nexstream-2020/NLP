{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f90e3b5",
   "metadata": {},
   "source": [
    "# Notebook 1: Tokenization for LLMs\n",
    "\n",
    "Welcome to the first notebook in our decoder-only transformer LLM course! In this notebook, we'll explore **tokenization** - the critical first step in processing text for language models.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand why tokenization is important for LLMs\n",
    "2. Learn the difference between character-level, word-level, and subword tokenization\n",
    "3. Implement Byte Pair Encoding (BPE) from scratch\n",
    "4. Train a tokenizer on sample text\n",
    "5. Use the Hugging Face tokenizers library\n",
    "6. Handle special tokens (BOS, EOS, PAD, UNK)\n",
    "7. Save and load tokenizers for reuse\n",
    "\n",
    "## üéØ Why Tokenization?\n",
    "\n",
    "Language models don't process raw text - they work with **numerical representations**. Tokenization is the process of:\n",
    "1. Breaking text into smaller units (tokens)\n",
    "2. Mapping each token to a unique integer ID\n",
    "3. Building a vocabulary of all possible tokens\n",
    "\n",
    "**Key Trade-offs:**\n",
    "- **Character-level**: Small vocabulary, long sequences (inefficient)\n",
    "- **Word-level**: Large vocabulary (out-of-vocabulary problems)\n",
    "- **Subword-level**: Best of both worlds! (BPE, WordPiece, Unigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 1. Character-level tokenization\n",
    "char_tokens = list(sample_text)\n",
    "print(f\"\\n1Ô∏è‚É£ CHARACTER-LEVEL ({len(char_tokens)} tokens):\")\n",
    "print(char_tokens[:30], \"...\")\n",
    "print(f\"   Vocabulary size: {len(set(char_tokens))}\")\n",
    "\n",
    "# 2. Word-level tokenization (naive split)\n",
    "word_tokens = sample_text.split()\n",
    "print(f\"\\n2Ô∏è‚É£ WORD-LEVEL ({len(word_tokens)} tokens):\")\n",
    "print(word_tokens)\n",
    "print(f\"   Vocabulary size: {len(set(word_tokens))}\")\n",
    "\n",
    "# 3. Simple subword tokenization (preview)\n",
    "# We'll implement this properly in the next section\n",
    "print(f\"\\n3Ô∏è‚É£ SUBWORD-LEVEL (BPE - coming up next!):\")\n",
    "print(\"   Example: 'jumping' ‚Üí ['jump', 'ing']\")\n",
    "print(\"   Balance between vocab size and sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    \"\"\"\n",
    "    A simple implementation of Byte Pair Encoding (BPE) tokenizer.\n",
    "    Educational implementation - not optimized for production use.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # token -> id\n",
    "        self.inverse_vocab = {}  # id -> token\n",
    "        self.merges = {}  # (token1, token2) -> merged_token\n",
    "        self.bpe_ranks = {}  # merged_token -> rank\n",
    "        \n",
    "    def train(self, texts: List[str], verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a list of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to train on\n",
    "            verbose: Print progress information\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"üîß Training BPE Tokenizer...\")\n",
    "            print(f\"Target vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "        # Step 1: Initialize with character-level vocabulary\n",
    "        # Start by splitting text into words and then characters\n",
    "        word_freqs = defaultdict(int)\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "        \n",
    "        # Convert words to character sequences with end-of-word marker\n",
    "        splits = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            splits[word] = list(word) + ['</w>']\n",
    "        \n",
    "        # Build initial character vocabulary\n",
    "        vocab = set()\n",
    "        for word in splits.values():\n",
    "            vocab.update(word)\n",
    "        \n",
    "        # Initialize vocab dict\n",
    "        self.vocab = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "        self.inverse_vocab = {idx: char for char, idx in self.vocab.items()}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "            print(f\"Sample characters: {list(self.vocab.keys())[:20]}\")\n",
    "        \n",
    "        # Step 2: Iteratively merge most frequent pairs\n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            # Count all pairs\n",
    "            pairs = defaultdict(int)\n",
    "            for word, word_split in splits.items():\n",
    "                for j in range(len(word_split) - 1):\n",
    "                    pairs[(word_split[j], word_split[j + 1])] += word_freqs[word]\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge the pair in all words\n",
    "            new_token = ''.join(best_pair)\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.bpe_ranks[new_token] = i\n",
    "            \n",
    "            # Update splits\n",
    "            new_splits = {}\n",
    "            for word, word_split in splits.items():\n",
    "                new_split = []\n",
    "                j = 0\n",
    "                while j < len(word_split):\n",
    "                    if j < len(word_split) - 1 and \\\n",
    "                       (word_split[j], word_split[j + 1]) == best_pair:\n",
    "                        new_split.append(new_token)\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        new_split.append(word_split[j])\n",
    "                        j += 1\n",
    "                new_splits[word] = new_split\n",
    "            splits = new_splits\n",
    "            \n",
    "            # Add new token to vocabulary\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.inverse_vocab[len(self.inverse_vocab)] = new_token\n",
    "            \n",
    "            if verbose and (i + 1) % 50 == 0:\n",
    "                print(f\"  Merge {i+1}/{num_merges}: {best_pair[0]} + {best_pair[1]} ‚Üí {new_token}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Training complete! Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Split word into characters\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply learned merges\n",
    "            while len(word_tokens) > 1:\n",
    "                # Find pairs that can be merged\n",
    "                pairs = [(word_tokens[i], word_tokens[i + 1]) \n",
    "                        for i in range(len(word_tokens) - 1)]\n",
    "                \n",
    "                # Find the pair with the lowest rank (earliest merge)\n",
    "                valid_pairs = [(pair, self.bpe_ranks.get(self.merges.get(pair, ''), float('inf')))\n",
    "                              for pair in pairs if pair in self.merges]\n",
    "                \n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "                \n",
    "                # Merge the best pair\n",
    "                best_pair = min(valid_pairs, key=lambda x: x[1])[0]\n",
    "                merged = self.merges[best_pair]\n",
    "                \n",
    "                # Apply merge\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if i < len(word_tokens) - 1 and \\\n",
    "                       (word_tokens[i], word_tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_tokens\n",
    "            \n",
    "            # Convert to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    tokens.append(self.vocab[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens (shouldn't happen in training data)\n",
    "                    tokens.append(self.vocab.get('<UNK>', 0))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        tokens = [self.inverse_vocab[tid] for tid in token_ids if tid in self.inverse_vocab]\n",
    "        text = ''.join(tokens).replace('</w>', ' ')\n",
    "        return text.strip()\n",
    "\n",
    "# Create an instance\n",
    "tokenizer = SimpleBPETokenizer(vocab_size=300)\n",
    "print(\"‚úÖ SimpleBPETokenizer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd83c76",
   "metadata": {},
   "source": [
    "## üéØ Summary & Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "‚úÖ Different tokenization approaches (character, word, subword)  \n",
    "‚úÖ Byte Pair Encoding (BPE) algorithm  \n",
    "‚úÖ Implemented BPE from scratch  \n",
    "‚úÖ Used Hugging Face tokenizers for production  \n",
    "‚úÖ Special tokens (PAD, UNK, BOS, EOS)  \n",
    "‚úÖ Saving and loading tokenizers  \n",
    "\n",
    "### Key Takeaways:\n",
    "- **Subword tokenization** (BPE) balances vocabulary size and sequence length\n",
    "- **Special tokens** are essential for model training\n",
    "- **Hugging Face tokenizers** provide fast, production-ready implementations\n",
    "\n",
    "### Next Notebook: `02_embeddings.ipynb`\n",
    "In the next notebook, we'll convert token IDs into dense vector representations using **embedding layers** and **positional encodings**!\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercises (Optional):\n",
    "\n",
    "1. **Train a larger tokenizer**: Use a real dataset (e.g., load text from a file)\n",
    "2. **Compare vocab sizes**: Train tokenizers with different vocabulary sizes (100, 500, 5000) and compare\n",
    "3. **Visualize token frequencies**: Create a histogram of token frequencies\n",
    "4. **Domain-specific tokenizer**: Train a tokenizer on biomedical or chemical text\n",
    "5. **Token length analysis**: Analyze average token length vs vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddee4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving tokenizers\n",
    "tokenizer_dir = \"../data/tokenizers\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Train on a larger corpus (you would load your actual dataset here)\n",
    "sample_corpus = training_corpus * 100  # Repeat for demo\n",
    "\n",
    "# Train the HF tokenizer\n",
    "print(\"Training Hugging Face tokenizer...\")\n",
    "hf_tokenizer.train_from_iterator(sample_corpus, trainer=trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_path = os.path.join(tokenizer_dir, \"bpe_tokenizer.json\")\n",
    "hf_tokenizer.save(tokenizer_path)\n",
    "print(f\"‚úÖ Tokenizer saved to: {tokenizer_path}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(\"‚úÖ Tokenizer loaded successfully!\")\n",
    "\n",
    "# Test it\n",
    "test_text = \"The transformer architecture revolutionized NLP!\"\n",
    "encoding = loaded_tokenizer.encode(test_text)\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Token IDs: {encoding.ids}\")\n",
    "print(f\"Tokens: {encoding.tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea73c9",
   "metadata": {},
   "source": [
    "## Part 5: Saving and Loading Tokenizers\n",
    "\n",
    "For our LLM project, we'll need to save trained tokenizers and reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1857952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed (uncomment)\n",
    "# !pip install tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "def create_bpe_tokenizer(vocab_size=1000):\n",
    "    \"\"\"Create a BPE tokenizer using Hugging Face tokenizers library.\"\"\"\n",
    "    \n",
    "    # Initialize a tokenizer with BPE model\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<UNK>\"))\n",
    "    \n",
    "    # Set up pre-tokenization (how to split text before applying BPE)\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Set up decoder\n",
    "    tokenizer.decoder = decoders.BPEDecoder()\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"],\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer, trainer\n",
    "\n",
    "# Create tokenizer\n",
    "hf_tokenizer, trainer = create_bpe_tokenizer(vocab_size=500)\n",
    "\n",
    "print(\"‚úÖ Hugging Face tokenizer created!\")\n",
    "print(\"\\nSpecial tokens:\")\n",
    "print(\"  <PAD>: Padding token\")\n",
    "print(\"  <UNK>: Unknown token\")  \n",
    "print(\"  <BOS>: Beginning of sequence\")\n",
    "print(\"  <EOS>: End of sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f843fa",
   "metadata": {},
   "source": [
    "## Part 4: Using Hugging Face Tokenizers\n",
    "\n",
    "For production use, we'll use the Hugging Face `tokenizers` library, which is much faster and more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training corpus\n",
    "training_corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A journey of a thousand miles begins with a single step\",\n",
    "    \"To be or not to be that is the question\",\n",
    "    \"All that glitters is not gold\",\n",
    "    \"Better late than never\",\n",
    "    \"The early bird catches the worm\",\n",
    "    \"Actions speak louder than words\",\n",
    "]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(training_corpus, verbose=True)\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"The quick brown fox\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üß™ Test Encoding/Decoding\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Original:  {test_text}\")\n",
    "print(f\"Encoded:   {encoded}\")\n",
    "print(f\"Decoded:   {decoded}\")\n",
    "print(f\"Match:     {test_text.lower() == decoded.lower()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bacbe9",
   "metadata": {},
   "source": [
    "## Part 3: Training the BPE Tokenizer\n",
    "\n",
    "Let's train our tokenizer on some sample text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3319a9d",
   "metadata": {},
   "source": [
    "## Part 2: Byte Pair Encoding (BPE) from Scratch\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a subword tokenization algorithm that:\n",
    "1. Starts with a character-level vocabulary\n",
    "2. Iteratively merges the most frequent pair of consecutive tokens\n",
    "3. Stops when reaching a desired vocabulary size\n",
    "\n",
    "This creates a vocabulary that balances coverage and efficiency!\n",
    "\n",
    "### Algorithm Steps:\n",
    "1. Initialize vocabulary with all characters\n",
    "2. Count all adjacent token pairs\n",
    "3. Merge the most frequent pair\n",
    "4. Repeat until vocabulary size is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77993ef2",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Different Tokenization Approaches\n",
    "\n",
    "Let's start with a simple example to understand the three main approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
