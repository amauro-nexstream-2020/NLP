{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577b7f7b",
   "metadata": {},
   "source": [
    "# Embeddings in Transformers: A Comprehensive Guide\n",
    "\n",
    "This notebook provides a thorough guide to embeddings in transformer models, implementing them from scratch while referencing the nanochat architecture. We'll cover token embeddings, positional embeddings (including rotary embeddings), and the unembedding layer.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Embeddings](#introduction)\n",
    "2. [Token Embeddings](#token-embeddings)\n",
    "3. [Positional Embeddings](#positional-embeddings)\n",
    "4. [Rotary Position Embeddings](#rotary-embeddings)\n",
    "5. [Combined Embeddings](#combined-embeddings)\n",
    "6. [Unembedding Layer](#unembedding)\n",
    "7. [Implementation from Scratch](#implementation)\n",
    "8. [Visualization](#visualization)\n",
    "9. [Training a Simple Model](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f98bf",
   "metadata": {},
   "source": [
    "## Introduction to Embeddings\n",
    "\n",
    "Embeddings are the foundation of transformer models. They convert discrete tokens (words, subwords, or characters) into continuous vector representations that capture semantic meaning and relationships.\n",
    "\n",
    "In the nanochat architecture (a minimal LLM training codebase), embeddings serve three main purposes:\n",
    "1. **Token Embeddings (wte)**: Map tokens to dense vectors\n",
    "2. **Positional Embeddings**: Add position information to tokens\n",
    "3. **Unembedding (lm_head)**: Project back to vocabulary space for prediction\n",
    "\n",
    "Key insights from nanochat:\n",
    "- Token embeddings use `torch.nn.Embedding`\n",
    "- Positional information uses rotary embeddings (RoPE)\n",
    "- The unembedding layer is untied from token embeddings for better performance\n",
    "- Learning rates are scaled by `(n_embd/768)^-0.5` for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7017c",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f00795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38577b55",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "Token embeddings map discrete tokens to continuous vector representations. In nanochat, this is implemented using `torch.nn.Embedding`.\n",
    "\n",
    "The embedding layer learns a lookup table where each token ID maps to a dense vector of dimension `n_embd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary size and embedding dimension\n",
    "vocab_size = 1000  # Size of our vocabulary\n",
    "n_embd = 128       # Embedding dimension (nanochat uses various sizes)\n",
    "\n",
    "# Create token embedding layer (wte in nanochat)\n",
    "token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "token_embedding.to(device)\n",
    "\n",
    "print(f\"Token embedding shape: {token_embedding.weight.shape}\")\n",
    "print(f\"Number of parameters: {token_embedding.weight.numel()}\")\n",
    "\n",
    "# Example: Convert token IDs to embeddings\n",
    "token_ids = torch.tensor([[1, 5, 10, 25]], dtype=torch.long).to(device)\n",
    "embedded_tokens = token_embedding(token_ids)\n",
    "\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Embedded tokens shape: {embedded_tokens.shape}\")\n",
    "print(f\"Sample embedding values:\\n{embedded_tokens[0, :3, :5]}\")  # First 3 tokens, first 5 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e5d04",
   "metadata": {},
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "Traditional positional embeddings add absolute position information. However, nanochat uses **Rotary Position Embeddings (RoPE)**, which encode relative positional relationships more effectively.\n",
    "\n",
    "RoPE applies rotation matrices to the embedding vectors based on their positions, allowing the model to better capture relative distances between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104d7a6",
   "metadata": {},
   "source": [
    "## Rotary Position Embeddings (RoPE)\n",
    "\n",
    "RoPE rotates embedding vectors based on their positions using precomputed cosine and sine values. This allows the attention mechanism to capture relative positional relationships.\n",
    "\n",
    "The key insight: Instead of adding positional embeddings, we rotate the token embeddings using position-dependent rotation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with cos and sin.\n",
    "    This is used in rotary position embeddings.\n",
    "\n",
    "    Args:\n",
    "        dim: Dimension of the embedding\n",
    "        end: Maximum sequence length\n",
    "        theta: Base for the frequency calculation\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(tensor: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary position embedding to the input tensor.\n",
    "\n",
    "    Args:\n",
    "        tensor: Input tensor of shape (batch_size, seq_len, n_heads, head_dim)\n",
    "        freqs_cis: Precomputed frequencies of shape (seq_len, head_dim//2)\n",
    "    \"\"\"\n",
    "    # Convert to complex\n",
    "    x_complex = torch.view_as_complex(tensor.float().reshape(*tensor.shape[:-1], -1, 2))\n",
    "\n",
    "    # Apply rotation\n",
    "    x_rotated = x_complex * freqs_cis.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Convert back to real\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(tensor.shape)\n",
    "    return x_out.type_as(tensor)\n",
    "\n",
    "# Example usage\n",
    "max_seq_len = 100\n",
    "freqs_cis = precompute_freqs_cis(n_embd // 2, max_seq_len)  # For head_dim = n_embd // n_heads, but simplified\n",
    "\n",
    "print(f\"Frequency cis shape: {freqs_cis.shape}\")\n",
    "print(f\"Sample frequency values: {freqs_cis[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60a153",
   "metadata": {},
   "source": [
    "## Combined Embeddings\n",
    "\n",
    "In the full transformer architecture, token embeddings and positional embeddings are combined. In nanochat, this happens within the attention mechanism where RoPE is applied to the query and key vectors.\n",
    "\n",
    "For simplicity, let's show how embeddings are combined before being fed into the transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple embedding layer that combines token and positional embeddings\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Precompute rotary frequencies (simplified for demonstration)\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(n_embd, max_seq_len))\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        # Get token embeddings\n",
    "        token_embeds = self.token_embedding(token_ids)  # (batch_size, seq_len, n_embd)\n",
    "\n",
    "        # Apply rotary position embeddings\n",
    "        # In practice, this is done in the attention mechanism, but we show it here for illustration\n",
    "        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        freqs_cis = self.freqs_cis[positions]  # (1, seq_len, n_embd//2)\n",
    "\n",
    "        # Reshape for rotary application (simulating multi-head attention format)\n",
    "        token_embeds_reshaped = token_embeds.view(batch_size, seq_len, 1, n_embd)\n",
    "        rotated_embeds = apply_rotary_emb(token_embeds_reshaped, freqs_cis)\n",
    "        final_embeds = rotated_embeds.view(batch_size, seq_len, n_embd)\n",
    "\n",
    "        return final_embeds\n",
    "\n",
    "# Test the combined embedding layer\n",
    "embedding_layer = EmbeddingLayer(vocab_size, n_embd)\n",
    "embedding_layer.to(device)\n",
    "\n",
    "# Test with sample input\n",
    "sample_input = torch.tensor([[1, 5, 10, 25, 100]], dtype=torch.long).to(device)\n",
    "embedded_output = embedding_layer(sample_input)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {embedded_output.shape}\")\n",
    "print(f\"Sample embedded values:\\n{embedded_output[0, :3, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d7bb6",
   "metadata": {},
   "source": [
    "## Unembedding Layer (lm_head)\n",
    "\n",
    "The unembedding layer projects the final transformer outputs back to vocabulary space for next-token prediction. In nanochat, this is implemented as a separate linear layer (`lm_head`) that is **untied** from the token embeddings (`wte`).\n",
    "\n",
    "Key points from nanochat:\n",
    "- `lm_head` is `torch.nn.Linear(n_embd, vocab_size)`\n",
    "- It's initialized with zeros (not random weights)\n",
    "- It uses the same learning rate scaling as token embeddings: `(n_embd/768)^-0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cfc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unembedding layer (lm_head in nanochat)\n",
    "lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "lm_head.to(device)\n",
    "\n",
    "# Initialize weights to zeros as in nanochat\n",
    "with torch.no_grad():\n",
    "    lm_head.weight.fill_(0.0)\n",
    "\n",
    "print(f\"Unembedding layer shape: {lm_head.weight.shape}\")\n",
    "print(f\"Number of parameters: {lm_head.weight.numel()}\")\n",
    "\n",
    "# Test unembedding\n",
    "# Simulate transformer output (normally this would come from the transformer layers)\n",
    "transformer_output = torch.randn(1, 5, n_embd).to(device)  # batch_size=1, seq_len=5\n",
    "\n",
    "# Get logits for next token prediction\n",
    "logits = lm_head(transformer_output)\n",
    "print(f\"Logits shape: {logits.shape}\")  # Should be (batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Get predicted token IDs\n",
    "predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "print(f\"Predicted tokens: {predicted_tokens}\")\n",
    "\n",
    "# Get probabilities\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Sample probabilities for first position:\\n{probabilities[0, 0, :10]}\")  # First 10 vocab items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b51697",
   "metadata": {},
   "source": [
    "## Complete Implementation from Scratch\n",
    "\n",
    "Let's create a complete embedding system that mirrors nanochat's architecture, including proper initialization and optimizer setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoChatEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete embedding system inspired by nanochat's architecture.\n",
    "    Includes token embeddings (wte), rotary position embeddings, and unembedding (lm_head).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, max_seq_len=1024):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Token embeddings (wte)\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Unembedding layer (lm_head) - untied from wte\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # Precompute rotary frequencies\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(n_embd, max_seq_len))\n",
    "\n",
    "        # Initialize lm_head weights to zeros (nanochat style)\n",
    "        with torch.no_grad():\n",
    "            self.lm_head.weight.fill_(0.0)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through embeddings.\n",
    "\n",
    "        Args:\n",
    "            token_ids: Input token IDs (batch_size, seq_len)\n",
    "            targets: Target token IDs for loss computation (optional)\n",
    "\n",
    "        Returns:\n",
    "            logits: Prediction logits (batch_size, seq_len, vocab_size)\n",
    "            loss: Cross-entropy loss (if targets provided)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        # Get token embeddings\n",
    "        token_embeds = self.wte(token_ids)  # (batch_size, seq_len, n_embd)\n",
    "\n",
    "        # Apply rotary position embeddings (simplified - normally done in attention)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        freqs_cis = self.freqs_cis[positions]\n",
    "\n",
    "        # Reshape for rotary application\n",
    "        token_embeds_reshaped = token_embeds.view(batch_size, seq_len, 1, n_embd)\n",
    "        rotated_embeds = apply_rotary_emb(token_embeds_reshaped, freqs_cis)\n",
    "        final_embeds = rotated_embeds.view(batch_size, seq_len, n_embd)\n",
    "\n",
    "        # Get logits through unembedding layer\n",
    "        logits = self.lm_head(final_embeds)\n",
    "\n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = nn.functional.cross_entropy(\n",
    "                logits.view(-1, self.vocab_size),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_learning_rate_scale(self):\n",
    "        \"\"\"\n",
    "        Get learning rate scaling factor for embeddings (nanochat style).\n",
    "        Scales by (n_embd/768)^-0.5\n",
    "        \"\"\"\n",
    "        return (self.n_embd / 768.0) ** -0.5\n",
    "\n",
    "# Test the complete implementation\n",
    "model = NanoChatEmbeddings(vocab_size, n_embd)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Learning rate scale: {model.get_learning_rate_scale():.4f}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, vocab_size, (2, 10)).to(device)  # batch_size=2, seq_len=10\n",
    "test_targets = torch.randint(0, vocab_size, (2, 10)).to(device)\n",
    "\n",
    "logits, loss = model(test_input, test_targets)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e847dc",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize the embeddings to understand how they capture semantic relationships. We'll use PCA and t-SNE to reduce the high-dimensional embeddings to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample embeddings for visualization\n",
    "sample_tokens = torch.arange(min(50, vocab_size)).to(device)  # First 50 tokens\n",
    "embeddings = model.wte(sample_tokens).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embeddings shape for visualization: {embeddings.shape}\")\n",
    "\n",
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot PCA\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], alpha=0.7)\n",
    "for i, (x, y) in enumerate(embeddings_pca):\n",
    "    plt.annotate(str(i), (x, y), fontsize=8, alpha=0.7)\n",
    "plt.title('Token Embeddings (PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.7)\n",
    "for i, (x, y) in enumerate(embeddings_tsne):\n",
    "    plt.annotate(str(i), (x, y), fontsize=8, alpha=0.7)\n",
    "plt.title('Token Embeddings (t-SNE)')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show explained variance for PCA\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"PCA explained variance: PC1={explained_variance[0]:.3f}, PC2={explained_variance[1]:.3f}\")\n",
    "print(f\"Total explained variance: {explained_variance.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01ffc9",
   "metadata": {},
   "source": [
    "## Training a Simple Model\n",
    "\n",
    "Let's train our embedding model on a simple next-token prediction task. We'll use the same optimizer setup as nanochat: AdamW for embeddings with learning rate scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "learning_rate_scale = model.get_learning_rate_scale()\n",
    "base_lr = 0.0004  # Base learning rate from nanochat\n",
    "learning_rate = base_lr * learning_rate_scale\n",
    "\n",
    "print(f\"Base learning rate: {base_lr}\")\n",
    "print(f\"Learning rate scale: {learning_rate_scale:.4f}\")\n",
    "print(f\"Final learning rate: {learning_rate:.6f}\")\n",
    "\n",
    "# Create optimizer (AdamW for embeddings as in nanochat)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate some training data (random for demonstration)\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "num_batches = 100\n",
    "\n",
    "print(\"Training embeddings...\")\n",
    "\n",
    "losses = []\n",
    "for batch_idx in range(num_batches):\n",
    "    # Generate random batch\n",
    "    input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "    target_tokens = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(input_tokens, target_tokens)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (batch_idx + 1) % 20 == 0:\n",
    "        print(f\"Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Embedding Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Loss reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508d6e0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a complete embedding system inspired by nanochat's architecture:\n",
    "\n",
    "### Key Components Implemented:\n",
    "1. **Token Embeddings (wte)**: `nn.Embedding` layer mapping tokens to dense vectors\n",
    "2. **Rotary Position Embeddings (RoPE)**: Position-dependent rotations for relative positional encoding\n",
    "3. **Unembedding Layer (lm_head)**: Linear projection back to vocabulary space, untied from token embeddings\n",
    "4. **Proper Initialization**: lm_head weights initialized to zeros\n",
    "5. **Optimizer Setup**: AdamW with learning rate scaling `(n_embd/768)^-0.5`\n",
    "\n",
    "### Key Insights from Nanochat:\n",
    "- **Untied Embeddings**: lm_head is separate from wte for better performance\n",
    "- **RoPE over Absolute PE**: Rotary embeddings capture relative relationships more effectively\n",
    "- **Zero Initialization**: lm_head starts with zero weights\n",
    "- **Scaled Learning Rates**: Embeddings use scaled learning rates based on embedding dimension\n",
    "\n",
    "### Applications:\n",
    "- **Language Modeling**: Next-token prediction\n",
    "- **Text Classification**: Using [CLS] token embeddings\n",
    "- **Semantic Search**: Finding similar texts in embedding space\n",
    "- **Transfer Learning**: Fine-tuning pretrained embeddings\n",
    "\n",
    "This foundation sets the stage for building complete transformer models, as explored in the subsequent notebooks in this series."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
