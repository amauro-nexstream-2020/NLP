{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05. The Decoder-Only Transformer Model\n",
                "\n",
                "We have built all the necessary components:\n",
                "1.  **Tokenizer** (Notebook 01)\n",
                "2.  **Embeddings** (Notebook 02)\n",
                "3.  **Attention** (Notebook 03)\n",
                "4.  **Transformer Block** (Notebook 04)\n",
                "\n",
                "Now, we will assemble these pieces into the full **Decoder-Only Transformer** architecture, similar to GPT-2.\n",
                "\n",
                "## Architecture Overview\n",
                "\n",
                "The full model consists of:\n",
                "1.  **Token Embeddings**: Map token IDs to vectors.\n",
                "2.  **Positional Embeddings**: Add position information.\n",
                "3.  **Dropout**: Applied to the sum of embeddings.\n",
                "4.  **Stack of Transformer Blocks**: The core processing units.\n",
                "5.  **Final LayerNorm**: Stabilize the output.\n",
                "6.  **Language Modeling Head**: Project back to vocabulary size to get logits.\n",
                "\n",
                "## Weight Tying\n",
                "A common practice in modern LLMs (like GPT-2) is to **tie the weights** of the token embedding layer and the language modeling head. This means `lm_head.weight = token_emb.weight`. This reduces the number of parameters and often improves performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, Tuple\n",
                "\n",
                "# Re-defining Config and Components for self-containment\n",
                "@dataclass\n",
                "class ModelConfig:\n",
                "    n_embd: int = 768\n",
                "    n_head: int = 12\n",
                "    n_layer: int = 12\n",
                "    n_positions: int = 1024\n",
                "    vocab_size: int = 50257\n",
                "    dropout: float = 0.1\n",
                "    bias: bool = True\n",
                "\n",
                "# ... (Paste MultiHeadAttention, FeedForward, TransformerBlock classes here if not importing)\n",
                "# For this notebook, we assume they are available or we re-define them briefly for the class structure.\n",
                "# To keep the notebook clean, we will assume the previous classes are defined or imported.\n",
                "# Ideally, we would move them to `src/model.py` and import them, but for the notebook flow, we'll redefine the Block.\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        assert config.n_embd % config.n_head == 0\n",
                "        self.n_head = config.n_head\n",
                "        self.n_embd = config.n_embd\n",
                "        self.head_dim = config.n_embd // config.n_head\n",
                "        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
                "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
                "        self.attn_dropout = nn.Dropout(config.dropout)\n",
                "        self.resid_dropout = nn.Dropout(config.dropout)\n",
                "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.n_positions, config.n_positions)).view(1, 1, config.n_positions, config.n_positions))\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, T, C = x.size()\n",
                "        qkv = self.qkv_proj(x)\n",
                "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
                "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
                "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
                "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
                "        attn = F.softmax(attn, dim=-1)\n",
                "        attn = self.attn_dropout(attn)\n",
                "        y = attn @ v\n",
                "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
                "        y = self.resid_dropout(self.out_proj(y))\n",
                "        return y\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
                "        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
                "        self.dropout = nn.Dropout(config.dropout)\n",
                "    def forward(self, x):\n",
                "        return self.dropout(self.fc2(F.gelu(self.fc1(x))))\n",
                "\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
                "        self.attn = MultiHeadAttention(config)\n",
                "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
                "        self.ffn = FeedForward(config)\n",
                "    def forward(self, x):\n",
                "        x = x + self.attn(self.ln1(x))\n",
                "        x = x + self.ffn(self.ln2(x))\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The DecoderLM Class\n",
                "\n",
                "Here is the implementation of the full model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderLM(nn.Module):\n",
                "    \"\"\"Decoder-only transformer language model.\"\"\"\n",
                "    \n",
                "    def __init__(self, config: ModelConfig):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        \n",
                "        # Token embeddings\n",
                "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
                "        \n",
                "        # Positional embeddings\n",
                "        self.pos_emb = nn.Embedding(config.n_positions, config.n_embd)\n",
                "        \n",
                "        # Dropout\n",
                "        self.drop = nn.Dropout(config.dropout)\n",
                "        \n",
                "        # Transformer blocks\n",
                "        self.blocks = nn.ModuleList([\n",
                "            TransformerBlock(config) for _ in range(config.n_layer)\n",
                "        ])\n",
                "        \n",
                "        # Final layer norm\n",
                "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
                "        \n",
                "        # Output projection (language modeling head)\n",
                "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
                "        \n",
                "        # Weight tying (share weights between token embeddings and output projection)\n",
                "        self.lm_head.weight = self.token_emb.weight\n",
                "        \n",
                "        # Initialize weights\n",
                "        self.apply(self._init_weights)\n",
                "    \n",
                "    def _init_weights(self, module):\n",
                "        \"\"\"Initialize weights (GPT-2 style).\"\"\"\n",
                "        if isinstance(module, nn.Linear):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "            if module.bias is not None:\n",
                "                torch.nn.init.zeros_(module.bias)\n",
                "        elif isinstance(module, nn.Embedding):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        input_ids: torch.Tensor,\n",
                "        targets: Optional[torch.Tensor] = None\n",
                "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            input_ids: Input token IDs [batch, seq_len]\n",
                "            targets: Target token IDs for computing loss [batch, seq_len]\n",
                "        \n",
                "        Returns:\n",
                "            logits: Output logits [batch, seq_len, vocab_size]\n",
                "            loss: Cross-entropy loss (if targets provided)\n",
                "        \"\"\"\n",
                "        device = input_ids.device\n",
                "        b, t = input_ids.size()\n",
                "        \n",
                "        assert t <= self.config.n_positions, f\"Sequence length {t} exceeds maximum {self.config.n_positions}\"\n",
                "        \n",
                "        # Token embeddings\n",
                "        tok_emb = self.token_emb(input_ids)  # [batch, seq_len, n_embd]\n",
                "        \n",
                "        # Positional embeddings\n",
                "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # [1, seq_len]\n",
                "        pos_emb = self.pos_emb(pos)  # [1, seq_len, n_embd]\n",
                "        \n",
                "        # Combine embeddings\n",
                "        x = self.drop(tok_emb + pos_emb)\n",
                "        \n",
                "        # Apply transformer blocks\n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "        \n",
                "        # Final layer norm\n",
                "        x = self.ln_f(x)\n",
                "        \n",
                "        # Language modeling head\n",
                "        logits = self.lm_head(x)  # [batch, seq_len, vocab_size]\n",
                "        \n",
                "        # Compute loss if targets provided\n",
                "        loss = None\n",
                "        if targets is not None:\n",
                "            loss = F.cross_entropy(\n",
                "                logits.view(-1, logits.size(-1)),\n",
                "                targets.view(-1),\n",
                "                ignore_index=-1  # Ignore padding tokens\n",
                "            )\n",
                "        \n",
                "        return logits, loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verification\n",
                "\n",
                "Let's instantiate a small model and check its parameter count and forward pass."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model parameters: 541,184\n",
                        "Input shape: torch.Size([2, 32])\n",
                        "Logits shape: torch.Size([2, 32, 1000])\n",
                        "Verification successful!\n"
                    ]
                }
            ],
            "source": [
                "def count_parameters(model):\n",
                "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "# Create a small config for testing\n",
                "small_config = ModelConfig(\n",
                "    n_embd=128,\n",
                "    n_head=4,\n",
                "    n_layer=2,\n",
                "    n_positions=128,\n",
                "    vocab_size=1000\n",
                ")\n",
                "\n",
                "model = DecoderLM(small_config)\n",
                "print(f\"Model parameters: {count_parameters(model):,}\")\n",
                "\n",
                "# Dummy input\n",
                "x = torch.randint(0, small_config.vocab_size, (2, 32))\n",
                "logits, loss = model(x)\n",
                "\n",
                "print(\"Input shape:\", x.shape)\n",
                "print(\"Logits shape:\", logits.shape)\n",
                "assert logits.shape == (2, 32, 1000)\n",
                "print(\"Verification successful!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "NLP",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
